{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations_with_replacement\n",
    "from numpy.linalg import qr, norm\n",
    "import open3d as o3d\n",
    "import datetime\n",
    "from utils import Logger, mkdir_p, progress_bar, save_model, save_args, cal_loss\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from data import ModelNet40\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree = 10\n",
    "# d = 3\n",
    "# num_poly = math.comb(degree + d, d)\n",
    "# print('num_poly:', num_poly)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# exponents = []\n",
    "# # Generate all multi-indices with total degree up to 'degree'\n",
    "# for total_degree in range(degree + 1):\n",
    "#     for exps in combinations_with_replacement(range(d), total_degree):\n",
    "#         exp = torch.zeros(d, dtype=int, device=device)\n",
    "#         for var in exps:\n",
    "#             exp[var] += 1\n",
    "#         exponents.append(exp)\n",
    "# M = len(exponents)\n",
    "# print('Number of monomials:', M)\n",
    "# print('Monomials:', exponents[:10])\n",
    "\n",
    "# exponents = torch.zeros(num_poly, d, dtype=int, device=device)\n",
    "# i = 0\n",
    "# for total_degree in range(degree + 1):\n",
    "#     for exps in combinations_with_replacement(range(d), total_degree):\n",
    "#         for var in exps:\n",
    "#             exponents[i, var] += 1\n",
    "#         i += 1\n",
    "\n",
    "# print('Monomials:', exponents[:10])\n",
    "# print(exponents.shape, exponents.dtype, exponents.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def generate_exponents(d, degree):\n",
    "    \"\"\"\n",
    "    Generate all multi-indices with total degree up to 'degree' for d-dimensional points.\n",
    "    \n",
    "    Parameters:\n",
    "        d (int): The dimension of the points.\n",
    "        degree (int): The maximum degree of the monomials.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: The multi-indices of shape (num_poly, d).\n",
    "    \"\"\"\n",
    "    num_poly = math.comb(degree + d, d)\n",
    "    exponents = torch.zeros(num_poly, d, dtype=int)\n",
    "    i = 0\n",
    "    for total_degree in range(degree + 1):\n",
    "        for exps in combinations_with_replacement(range(d), total_degree):\n",
    "            for var in exps:\n",
    "                exponents[i, var] += 1\n",
    "            i += 1\n",
    "            \n",
    "    return exponents\n",
    "\n",
    "def generate_monomials_sequences_batch(X, exponents):\n",
    "    \"\"\"\n",
    "    Generate monomials given a point cloud and multi-indices.\n",
    "\n",
    "    Parameters:\n",
    "        X (ndarray): An array of shape (B, N, d) representing the point cloud.\n",
    "        exponents (ndarray): The multi-indices of shape (M, d).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Monomial sequences of shape (B, M).\n",
    "    \"\"\"\n",
    "    B, N, d = X.shape\n",
    "    device = X.device\n",
    "    exponents = exponents.to(device)\n",
    "    M = len(exponents)\n",
    "    # print(f'Number of monomials: {M}') # Number of polynomials: n1 + n2 + ... + n_d = degree; degree + d choose d; d number of dividers for an array in space R^d.\n",
    "    monomials = torch.ones(B, N, M, device=device)\n",
    "    for i, exp in enumerate(exponents):\n",
    "        monomials[:, :, i] = torch.prod(X ** exp, axis=2) # x1^exp1 * x2^exp2 * ... * xd^expd. e.g. x1^2 * x2^3 * x3^1 \\in R^3\n",
    "    return monomials.sum(dim=1) / N # (B, N, M) -> (B, M)\n",
    "\n",
    "def generate_chebyshev_polynomials_sequence_batch_old(X, exponents):\n",
    "    \"\"\"\n",
    "    Generate Chebyshev polynomials given a point cloud and multi-indices.\n",
    "\n",
    "    Parameters:\n",
    "        X (ndarray): An array of shape (B, N, d) representing the d-dimensional point cloud.\n",
    "        exponents (ndarray): The multi-indices of shape (M, d).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Chebyshev polynomial sequences of shape (B, M).\n",
    "    \"\"\"\n",
    "    B, N, d = X.shape\n",
    "    device = X.device\n",
    "    exponents = exponents.to(device)\n",
    "    # print(f'Number of Chebyshev polynomials: {M}') # Number of polynomials: n1 + n2 + ... + n_d = degree; degree + d choose d; d number of dividers for an array in space R^d.\n",
    "    cheby_polynomials = torch.ones(B, N, M, device=device)\n",
    "    for i, exp in enumerate(exponents):\n",
    "        cheby_polynomials[:, :, i] = torch.prod(torch.cos(exp * torch.acos(X)), axis=2)\n",
    "    \n",
    "    return cheby_polynomials.sum(dim=1) / N # (B, N, M) -> (B, M)\n",
    "\n",
    "def generate_chebyshev_polynomials_sequence_batch(X, exponents):\n",
    "    \"\"\"\n",
    "    Generate Chebyshev polynomials given a point cloud and multi-indices.\n",
    "\n",
    "    Parameters:\n",
    "        X (ndarray): An array of shape (B, N, d) representing the d-dimensional point cloud.\n",
    "        exponents (ndarray): The multi-indices of shape (M, d).\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Chebyshev polynomial sequences of shape (B, M).\n",
    "    \"\"\"\n",
    "    B, N, d = X.shape\n",
    "    device = X.device\n",
    "    exponents = exponents.to(device)\n",
    "    cheby_polynomials = torch.cos(exponents.unsqueeze(0).unsqueeze(0) * torch.acos(X).unsqueeze(2)) # (B, N, M)\n",
    "    cheby_polynomials = cheby_polynomials.prod(dim=-1) # (B, N)\n",
    "    \n",
    "    return cheby_polynomials.sum(dim=1) / N # (B, N, M) -> (B, M)\n",
    "\n",
    "def poly_seq_batch(X, exponents, poly_type='monomial'):\n",
    "    if poly_type == 'monomial':\n",
    "        return generate_monomials_sequences_batch(X, exponents)\n",
    "    elif poly_type == 'chebyshev':\n",
    "        return generate_chebyshev_polynomials_sequence_batch(X, exponents)\n",
    "    else:\n",
    "        raise ValueError('Unknown polynomial type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exponents = generate_exponents(d, degree)\n",
    "# print('exponents:', exponents.shape, exponents[:10])\n",
    "\n",
    "# X = torch.rand(2, 100, 3)\n",
    "# polys = generate_chebyshev_polynomials_sequence_batch(X, exponents)\n",
    "# print('polys:', polys.shape, polys[0, :10])\n",
    "\n",
    "# polys = generate_chebyshev_polynomials_sequence_batch_old(X, exponents)\n",
    "# print('polys:', polys.shape, polys[0, :10])\n",
    "\n",
    "# # time the above functions\n",
    "# import time\n",
    "\n",
    "# X = torch.rand(2, 100, 3)\n",
    "# start = time.time()\n",
    "# polys = generate_chebyshev_polynomials_sequence_batch(X, exponents)\n",
    "# print('Time:', time.time() - start)\n",
    "\n",
    "# start = time.time()\n",
    "# polys = generate_chebyshev_polynomials_sequence_batch_old(X, exponents)\n",
    "# print('Time:', time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ModelNet40 as our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 2048\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(ModelNet40(partition='train', \n",
    "                                        num_points=num_points,\n",
    "                                        aug=True), \n",
    "                            num_workers=8,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', \n",
    "                                        num_points=num_points,\n",
    "                                        aug=False), \n",
    "                            num_workers=8,\n",
    "                            batch_size=batch_size // 2, \n",
    "                            shuffle=False, \n",
    "                            drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "class MLPClaasifier(nn.Module):\n",
    "    def __init__(self, dim_in, layer_dims, dim_out, dropout=0.5):\n",
    "        super(MLPClaasifier, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(nn.Sequential(\n",
    "            nn.Linear(dim_in, layer_dims[0]),\n",
    "            nn.BatchNorm1d(layer_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        ))\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Linear(layer_dims[i-1], layer_dims[i]),\n",
    "                nn.BatchNorm1d(layer_dims[i]),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "            ))\n",
    "        self.convs.append(nn.Linear(layer_dims[-1], dim_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x)\n",
    "        x = self.convs[-1](x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(32, 100) # 32 samples, 100 features\n",
    "model = MLPClaasifier(100, [128, 64], 40)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "class MomentNet(nn.Module):\n",
    "    def __init__(self, dim_in, layer_dims, dim_out, degree, poly_type='chebyshev'):\n",
    "        super(MomentNet, self).__init__()\n",
    "        self.poly_type = poly_type\n",
    "        self.exponts = generate_exponents(dim_in, degree)\n",
    "        self.num_poly = len(self.exponts)\n",
    "        self.mlp = MLPClaasifier(self.num_poly, layer_dims, dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = poly_seq_batch(x, self.exponts, self.poly_type)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(32, 2048, 3) # 32 samples, 2048 points, 3 features\n",
    "model = MomentNet(3, [128, 64], 40, 2, 'monomial')\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, criterion, epoch, num_epochs, device):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader))\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        logits = net(data)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "\n",
    "        train_true.append(label.cpu().numpy())\n",
    "        train_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += preds.eq(label).sum().item()\n",
    "\n",
    "        pbar.set_description(f'({epoch:3d}/{num_epochs:3d}) Loss: {train_loss / (batch_idx + 1):.3f} | Acc: {100. * correct / total:.3f}% ({correct}/{total})')\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    train_true = np.concatenate(train_true)\n",
    "    train_pred = np.concatenate(train_pred)\n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (train_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(train_true, train_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(train_true, train_pred))),\n",
    "        \"time\": time_cost\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(net, testloader, criterion, epoch, num_epochs, device):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(testloader), total=len(testloader))\n",
    "        for batch_idx, (data, label) in pbar:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            logits = net(data)\n",
    "            loss = criterion(logits, label)\n",
    "            test_loss += loss.item()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "            total += label.size(0)\n",
    "            correct += preds.eq(label).sum().item()\n",
    "            pbar.set_description(f'({epoch:3d}/{num_epochs:3d}) Loss: {test_loss / (batch_idx + 1):.3f} | Acc: {100. * correct / total:.3f}% ({correct}/{total})')\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (test_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(test_true, test_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(test_true, test_pred))),\n",
    "        \"time\": time_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  0/100) Loss: 1.851 | Acc: 51.171% (5027/9824): 100%|██████████| 307/307 [00:03<00:00, 87.09it/s] \n",
      "(  0/100) Loss: 1.174 | Acc: 68.233% (1684/2468): 100%|██████████| 155/155 [00:00<00:00, 189.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.851 acc_avg:33.017% acc:51.171% time:3s\n",
      "Testing loss:1.174 acc_avg:54.632% acc:68.233% time:1s [best test acc: 68.233%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  1/100) Loss: 1.292 | Acc: 64.007% (6288/9824): 100%|██████████| 307/307 [00:02<00:00, 107.56it/s]\n",
      "(  1/100) Loss: 0.933 | Acc: 74.514% (1839/2468): 100%|██████████| 155/155 [00:00<00:00, 190.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.292 acc_avg:48.703% acc:64.007% time:3s\n",
      "Testing loss:0.933 acc_avg:62.655% acc:74.514% time:1s [best test acc: 74.514%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  2/100) Loss: 1.109 | Acc: 68.149% (6695/9824): 100%|██████████| 307/307 [00:02<00:00, 107.75it/s]\n",
      "(  2/100) Loss: 0.810 | Acc: 76.864% (1897/2468): 100%|██████████| 155/155 [00:00<00:00, 186.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.109 acc_avg:54.736% acc:68.149% time:3s\n",
      "Testing loss:0.81 acc_avg:65.395% acc:76.864% time:1s [best test acc: 76.864%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  3/100) Loss: 1.004 | Acc: 71.661% (7040/9824): 100%|██████████| 307/307 [00:02<00:00, 108.85it/s]\n",
      "(  3/100) Loss: 0.780 | Acc: 77.553% (1914/2468): 100%|██████████| 155/155 [00:00<00:00, 190.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.004 acc_avg:59.373% acc:71.661% time:3s\n",
      "Testing loss:0.78 acc_avg:66.051% acc:77.553% time:1s [best test acc: 77.553%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  4/100) Loss: 0.926 | Acc: 73.819% (7252/9824): 100%|██████████| 307/307 [00:02<00:00, 108.05it/s]\n",
      "(  4/100) Loss: 0.774 | Acc: 78.404% (1935/2468): 100%|██████████| 155/155 [00:00<00:00, 192.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.926 acc_avg:62.328% acc:73.819% time:3s\n",
      "Testing loss:0.774 acc_avg:67.445% acc:78.404% time:1s [best test acc: 78.404%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  5/100) Loss: 0.870 | Acc: 75.336% (7401/9824): 100%|██████████| 307/307 [00:02<00:00, 105.74it/s]\n",
      "(  5/100) Loss: 0.676 | Acc: 80.632% (1990/2468): 100%|██████████| 155/155 [00:00<00:00, 183.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.87 acc_avg:64.474% acc:75.336% time:3s\n",
      "Testing loss:0.676 acc_avg:70.108% acc:80.632% time:1s [best test acc: 80.632%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  6/100) Loss: 0.832 | Acc: 75.886% (7455/9824): 100%|██████████| 307/307 [00:02<00:00, 106.86it/s]\n",
      "(  6/100) Loss: 0.693 | Acc: 80.105% (1977/2468): 100%|██████████| 155/155 [00:00<00:00, 194.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.832 acc_avg:65.123% acc:75.886% time:3s\n",
      "Testing loss:0.693 acc_avg:70.341% acc:80.105% time:1s [best test acc: 80.632%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  7/100) Loss: 0.800 | Acc: 77.138% (7578/9824): 100%|██████████| 307/307 [00:02<00:00, 107.07it/s]\n",
      "(  7/100) Loss: 0.682 | Acc: 81.483% (2011/2468): 100%|██████████| 155/155 [00:00<00:00, 190.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.8 acc_avg:67.019% acc:77.138% time:3s\n",
      "Testing loss:0.682 acc_avg:71.641% acc:81.483% time:1s [best test acc: 81.483%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  8/100) Loss: 0.753 | Acc: 78.115% (7674/9824): 100%|██████████| 307/307 [00:02<00:00, 107.64it/s]\n",
      "(  8/100) Loss: 0.656 | Acc: 81.280% (2006/2468): 100%|██████████| 155/155 [00:00<00:00, 192.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.753 acc_avg:68.248% acc:78.115% time:3s\n",
      "Testing loss:0.656 acc_avg:72.934% acc:81.28% time:1s [best test acc: 81.483%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  9/100) Loss: 0.732 | Acc: 78.664% (7728/9824): 100%|██████████| 307/307 [00:02<00:00, 107.32it/s]\n",
      "(  9/100) Loss: 0.682 | Acc: 82.212% (2029/2468): 100%|██████████| 155/155 [00:00<00:00, 188.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.732 acc_avg:69.063% acc:78.664% time:3s\n",
      "Testing loss:0.682 acc_avg:75.358% acc:82.212% time:1s [best test acc: 82.212%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 10/100) Loss: 0.699 | Acc: 79.275% (7788/9824): 100%|██████████| 307/307 [00:02<00:00, 107.77it/s]\n",
      "( 10/100) Loss: 0.638 | Acc: 81.645% (2015/2468): 100%|██████████| 155/155 [00:00<00:00, 189.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.699 acc_avg:69.829% acc:79.275% time:3s\n",
      "Testing loss:0.638 acc_avg:72.309% acc:81.645% time:1s [best test acc: 82.212%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 11/100) Loss: 0.663 | Acc: 80.568% (7915/9824): 100%|██████████| 307/307 [00:02<00:00, 107.95it/s]\n",
      "( 11/100) Loss: 0.683 | Acc: 81.280% (2006/2468): 100%|██████████| 155/155 [00:00<00:00, 193.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.663 acc_avg:71.605% acc:80.568% time:3s\n",
      "Testing loss:0.683 acc_avg:73.291% acc:81.28% time:1s [best test acc: 82.212%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 12/100) Loss: 0.655 | Acc: 81.057% (7963/9824): 100%|██████████| 307/307 [00:02<00:00, 106.50it/s]\n",
      "( 12/100) Loss: 0.648 | Acc: 82.496% (2036/2468): 100%|██████████| 155/155 [00:00<00:00, 188.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.655 acc_avg:72.564% acc:81.057% time:3s\n",
      "Testing loss:0.648 acc_avg:74.394% acc:82.496% time:1s [best test acc: 82.496%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 13/100) Loss: 0.630 | Acc: 81.759% (8032/9824): 100%|██████████| 307/307 [00:02<00:00, 108.88it/s]\n",
      "( 13/100) Loss: 0.627 | Acc: 83.630% (2064/2468): 100%|██████████| 155/155 [00:00<00:00, 191.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.63 acc_avg:72.946% acc:81.759% time:3s\n",
      "Testing loss:0.627 acc_avg:75.253% acc:83.63% time:1s [best test acc: 83.63%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 14/100) Loss: 0.603 | Acc: 82.105% (8066/9824): 100%|██████████| 307/307 [00:02<00:00, 108.56it/s]\n",
      "( 14/100) Loss: 0.632 | Acc: 83.387% (2058/2468): 100%|██████████| 155/155 [00:00<00:00, 187.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.603 acc_avg:74.011% acc:82.105% time:3s\n",
      "Testing loss:0.632 acc_avg:75.624% acc:83.387% time:1s [best test acc: 83.63%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 15/100) Loss: 0.606 | Acc: 82.258% (8081/9824): 100%|██████████| 307/307 [00:02<00:00, 106.20it/s]\n",
      "( 15/100) Loss: 0.619 | Acc: 83.387% (2058/2468): 100%|██████████| 155/155 [00:00<00:00, 192.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.606 acc_avg:74.217% acc:82.258% time:3s\n",
      "Testing loss:0.619 acc_avg:74.965% acc:83.387% time:1s [best test acc: 83.63%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 16/100) Loss: 0.578 | Acc: 82.940% (8148/9824): 100%|██████████| 307/307 [00:02<00:00, 106.27it/s]\n",
      "( 16/100) Loss: 0.648 | Acc: 82.496% (2036/2468): 100%|██████████| 155/155 [00:00<00:00, 189.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.578 acc_avg:75.06% acc:82.94% time:3s\n",
      "Testing loss:0.648 acc_avg:74.632% acc:82.496% time:1s [best test acc: 83.63%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 17/100) Loss: 0.554 | Acc: 83.133% (8167/9824): 100%|██████████| 307/307 [00:02<00:00, 108.06it/s]\n",
      "( 17/100) Loss: 0.621 | Acc: 84.076% (2075/2468): 100%|██████████| 155/155 [00:00<00:00, 176.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.554 acc_avg:75.527% acc:83.133% time:3s\n",
      "Testing loss:0.621 acc_avg:76.129% acc:84.076% time:1s [best test acc: 84.076%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 18/100) Loss: 0.555 | Acc: 83.622% (8215/9824): 100%|██████████| 307/307 [00:02<00:00, 107.94it/s]\n",
      "( 18/100) Loss: 0.627 | Acc: 84.481% (2085/2468): 100%|██████████| 155/155 [00:00<00:00, 187.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.555 acc_avg:75.566% acc:83.622% time:3s\n",
      "Testing loss:0.627 acc_avg:77.516% acc:84.481% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 19/100) Loss: 0.539 | Acc: 84.029% (8255/9824): 100%|██████████| 307/307 [00:02<00:00, 107.56it/s]\n",
      "( 19/100) Loss: 0.636 | Acc: 83.306% (2056/2468): 100%|██████████| 155/155 [00:00<00:00, 183.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.539 acc_avg:76.166% acc:84.029% time:3s\n",
      "Testing loss:0.636 acc_avg:76.883% acc:83.306% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 20/100) Loss: 0.509 | Acc: 84.823% (8333/9824): 100%|██████████| 307/307 [00:02<00:00, 107.14it/s]\n",
      "( 20/100) Loss: 0.618 | Acc: 83.752% (2067/2468): 100%|██████████| 155/155 [00:00<00:00, 191.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.509 acc_avg:77.687% acc:84.823% time:3s\n",
      "Testing loss:0.618 acc_avg:77.17% acc:83.752% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 21/100) Loss: 0.512 | Acc: 84.640% (8315/9824): 100%|██████████| 307/307 [00:02<00:00, 107.21it/s]\n",
      "( 21/100) Loss: 0.650 | Acc: 83.752% (2067/2468): 100%|██████████| 155/155 [00:00<00:00, 183.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.512 acc_avg:77.44% acc:84.64% time:3s\n",
      "Testing loss:0.65 acc_avg:77.411% acc:83.752% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 22/100) Loss: 0.493 | Acc: 84.823% (8333/9824): 100%|██████████| 307/307 [00:02<00:00, 108.04it/s]\n",
      "( 22/100) Loss: 0.712 | Acc: 83.468% (2060/2468): 100%|██████████| 155/155 [00:00<00:00, 188.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.493 acc_avg:77.535% acc:84.823% time:3s\n",
      "Testing loss:0.712 acc_avg:78.052% acc:83.468% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 23/100) Loss: 0.495 | Acc: 85.505% (8400/9824): 100%|██████████| 307/307 [00:02<00:00, 108.44it/s]\n",
      "( 23/100) Loss: 0.647 | Acc: 83.874% (2070/2468): 100%|██████████| 155/155 [00:00<00:00, 185.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.495 acc_avg:78.712% acc:85.505% time:3s\n",
      "Testing loss:0.647 acc_avg:78.024% acc:83.874% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 24/100) Loss: 0.475 | Acc: 85.464% (8396/9824): 100%|██████████| 307/307 [00:02<00:00, 107.42it/s]\n",
      "( 24/100) Loss: 0.677 | Acc: 83.387% (2058/2468): 100%|██████████| 155/155 [00:00<00:00, 189.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.475 acc_avg:78.777% acc:85.464% time:3s\n",
      "Testing loss:0.677 acc_avg:77.291% acc:83.387% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 25/100) Loss: 0.474 | Acc: 85.617% (8411/9824): 100%|██████████| 307/307 [00:02<00:00, 107.82it/s]\n",
      "( 25/100) Loss: 0.658 | Acc: 83.387% (2058/2468): 100%|██████████| 155/155 [00:00<00:00, 194.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.474 acc_avg:79.158% acc:85.617% time:3s\n",
      "Testing loss:0.658 acc_avg:76.257% acc:83.387% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 26/100) Loss: 0.450 | Acc: 85.963% (8445/9824): 100%|██████████| 307/307 [00:02<00:00, 107.86it/s]\n",
      "( 26/100) Loss: 0.631 | Acc: 84.279% (2080/2468): 100%|██████████| 155/155 [00:00<00:00, 192.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.45 acc_avg:79.247% acc:85.963% time:3s\n",
      "Testing loss:0.631 acc_avg:77.464% acc:84.279% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 27/100) Loss: 0.448 | Acc: 86.502% (8498/9824): 100%|██████████| 307/307 [00:02<00:00, 107.50it/s]\n",
      "( 27/100) Loss: 0.630 | Acc: 84.198% (2078/2468): 100%|██████████| 155/155 [00:00<00:00, 187.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.448 acc_avg:79.867% acc:86.502% time:3s\n",
      "Testing loss:0.63 acc_avg:77.227% acc:84.198% time:1s [best test acc: 84.481%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 28/100) Loss: 0.438 | Acc: 86.838% (8531/9824): 100%|██████████| 307/307 [00:02<00:00, 108.00it/s]\n",
      "( 28/100) Loss: 0.651 | Acc: 84.522% (2086/2468): 100%|██████████| 155/155 [00:00<00:00, 194.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.438 acc_avg:80.506% acc:86.838% time:3s\n",
      "Testing loss:0.651 acc_avg:78.349% acc:84.522% time:1s [best test acc: 84.522%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 29/100) Loss: 0.414 | Acc: 87.225% (8569/9824): 100%|██████████| 307/307 [00:02<00:00, 107.76it/s]\n",
      "( 29/100) Loss: 0.634 | Acc: 85.008% (2098/2468): 100%|██████████| 155/155 [00:00<00:00, 190.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.414 acc_avg:81.221% acc:87.225% time:3s\n",
      "Testing loss:0.634 acc_avg:78.808% acc:85.008% time:1s [best test acc: 85.008%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 30/100) Loss: 0.428 | Acc: 87.001% (8547/9824): 100%|██████████| 307/307 [00:02<00:00, 106.80it/s]\n",
      "( 30/100) Loss: 0.655 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:00<00:00, 187.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.428 acc_avg:80.807% acc:87.001% time:3s\n",
      "Testing loss:0.655 acc_avg:79.429% acc:85.049% time:1s [best test acc: 85.049%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 31/100) Loss: 0.418 | Acc: 87.449% (8591/9824): 100%|██████████| 307/307 [00:02<00:00, 106.97it/s]\n",
      "( 31/100) Loss: 0.606 | Acc: 85.130% (2101/2468): 100%|██████████| 155/155 [00:00<00:00, 191.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.418 acc_avg:81.893% acc:87.449% time:3s\n",
      "Testing loss:0.606 acc_avg:78.832% acc:85.13% time:1s [best test acc: 85.13%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 32/100) Loss: 0.395 | Acc: 87.826% (8628/9824): 100%|██████████| 307/307 [00:02<00:00, 108.12it/s]\n",
      "( 32/100) Loss: 0.627 | Acc: 84.968% (2097/2468): 100%|██████████| 155/155 [00:00<00:00, 179.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.395 acc_avg:82.228% acc:87.826% time:3s\n",
      "Testing loss:0.627 acc_avg:78.991% acc:84.968% time:1s [best test acc: 85.13%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 33/100) Loss: 0.384 | Acc: 87.836% (8629/9824): 100%|██████████| 307/307 [00:02<00:00, 107.60it/s]\n",
      "( 33/100) Loss: 0.681 | Acc: 84.117% (2076/2468): 100%|██████████| 155/155 [00:00<00:00, 187.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.384 acc_avg:82.071% acc:87.836% time:3s\n",
      "Testing loss:0.681 acc_avg:76.999% acc:84.117% time:1s [best test acc: 85.13%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 34/100) Loss: 0.392 | Acc: 88.009% (8646/9824): 100%|██████████| 307/307 [00:02<00:00, 108.49it/s]\n",
      "( 34/100) Loss: 0.655 | Acc: 84.238% (2079/2468): 100%|██████████| 155/155 [00:00<00:00, 187.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.392 acc_avg:82.513% acc:88.009% time:3s\n",
      "Testing loss:0.655 acc_avg:77.148% acc:84.238% time:1s [best test acc: 85.13%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 35/100) Loss: 0.382 | Acc: 88.406% (8685/9824): 100%|██████████| 307/307 [00:02<00:00, 108.35it/s]\n",
      "( 35/100) Loss: 0.643 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 187.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.382 acc_avg:82.998% acc:88.406% time:3s\n",
      "Testing loss:0.643 acc_avg:78.819% acc:85.332% time:1s [best test acc: 85.332%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 36/100) Loss: 0.374 | Acc: 88.569% (8701/9824): 100%|██████████| 307/307 [00:02<00:00, 106.92it/s]\n",
      "( 36/100) Loss: 0.645 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:00<00:00, 192.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.374 acc_avg:83.234% acc:88.569% time:3s\n",
      "Testing loss:0.645 acc_avg:79.469% acc:85.049% time:1s [best test acc: 85.332%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 37/100) Loss: 0.355 | Acc: 88.701% (8714/9824): 100%|██████████| 307/307 [00:02<00:00, 108.57it/s]\n",
      "( 37/100) Loss: 0.677 | Acc: 84.724% (2091/2468): 100%|██████████| 155/155 [00:00<00:00, 184.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.355 acc_avg:83.234% acc:88.701% time:3s\n",
      "Testing loss:0.677 acc_avg:78.233% acc:84.724% time:1s [best test acc: 85.332%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 38/100) Loss: 0.340 | Acc: 89.342% (8777/9824): 100%|██████████| 307/307 [00:02<00:00, 108.26it/s]\n",
      "( 38/100) Loss: 0.698 | Acc: 83.793% (2068/2468): 100%|██████████| 155/155 [00:00<00:00, 190.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.34 acc_avg:84.485% acc:89.342% time:3s\n",
      "Testing loss:0.698 acc_avg:77.106% acc:83.793% time:1s [best test acc: 85.332%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 39/100) Loss: 0.342 | Acc: 88.915% (8735/9824): 100%|██████████| 307/307 [00:02<00:00, 108.26it/s]\n",
      "( 39/100) Loss: 0.662 | Acc: 84.724% (2091/2468): 100%|██████████| 155/155 [00:00<00:00, 186.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.342 acc_avg:83.551% acc:88.915% time:3s\n",
      "Testing loss:0.662 acc_avg:78.791% acc:84.724% time:1s [best test acc: 85.332%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 40/100) Loss: 0.326 | Acc: 89.577% (8800/9824): 100%|██████████| 307/307 [00:02<00:00, 106.92it/s]\n",
      "( 40/100) Loss: 0.699 | Acc: 85.737% (2116/2468): 100%|██████████| 155/155 [00:00<00:00, 183.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.326 acc_avg:84.675% acc:89.577% time:3s\n",
      "Testing loss:0.699 acc_avg:79.815% acc:85.737% time:1s [best test acc: 85.737%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 41/100) Loss: 0.315 | Acc: 89.739% (8816/9824): 100%|██████████| 307/307 [00:02<00:00, 107.56it/s]\n",
      "( 41/100) Loss: 0.698 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 188.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.315 acc_avg:84.37% acc:89.739% time:3s\n",
      "Testing loss:0.698 acc_avg:80.196% acc:85.332% time:1s [best test acc: 85.737%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 42/100) Loss: 0.314 | Acc: 90.065% (8848/9824): 100%|██████████| 307/307 [00:02<00:00, 107.96it/s]\n",
      "( 42/100) Loss: 0.684 | Acc: 85.251% (2104/2468): 100%|██████████| 155/155 [00:00<00:00, 185.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.314 acc_avg:85.268% acc:90.065% time:3s\n",
      "Testing loss:0.684 acc_avg:79.687% acc:85.251% time:1s [best test acc: 85.737%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 43/100) Loss: 0.304 | Acc: 90.238% (8865/9824): 100%|██████████| 307/307 [00:02<00:00, 107.20it/s]\n",
      "( 43/100) Loss: 0.715 | Acc: 84.887% (2095/2468): 100%|██████████| 155/155 [00:00<00:00, 190.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.304 acc_avg:85.551% acc:90.238% time:3s\n",
      "Testing loss:0.715 acc_avg:79.008% acc:84.887% time:1s [best test acc: 85.737%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 44/100) Loss: 0.300 | Acc: 90.411% (8882/9824): 100%|██████████| 307/307 [00:02<00:00, 107.29it/s]\n",
      "( 44/100) Loss: 0.687 | Acc: 85.900% (2120/2468): 100%|██████████| 155/155 [00:00<00:00, 188.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.3 acc_avg:85.8% acc:90.411% time:3s\n",
      "Testing loss:0.687 acc_avg:80.274% acc:85.9% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 45/100) Loss: 0.301 | Acc: 90.544% (8895/9824): 100%|██████████| 307/307 [00:02<00:00, 107.54it/s]\n",
      "( 45/100) Loss: 0.704 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:00<00:00, 177.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.301 acc_avg:86.054% acc:90.544% time:3s\n",
      "Testing loss:0.704 acc_avg:79.362% acc:85.049% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 46/100) Loss: 0.292 | Acc: 90.594% (8900/9824): 100%|██████████| 307/307 [00:02<00:00, 106.17it/s]\n",
      "( 46/100) Loss: 0.717 | Acc: 84.400% (2083/2468): 100%|██████████| 155/155 [00:00<00:00, 185.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.292 acc_avg:86.196% acc:90.594% time:3s\n",
      "Testing loss:0.717 acc_avg:79.07% acc:84.4% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 47/100) Loss: 0.274 | Acc: 91.164% (8956/9824): 100%|██████████| 307/307 [00:02<00:00, 107.19it/s]\n",
      "( 47/100) Loss: 0.735 | Acc: 84.806% (2093/2468): 100%|██████████| 155/155 [00:00<00:00, 183.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.274 acc_avg:87.127% acc:91.164% time:3s\n",
      "Testing loss:0.735 acc_avg:78.508% acc:84.806% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 48/100) Loss: 0.271 | Acc: 91.297% (8969/9824): 100%|██████████| 307/307 [00:02<00:00, 107.02it/s]\n",
      "( 48/100) Loss: 0.696 | Acc: 84.968% (2097/2468): 100%|██████████| 155/155 [00:00<00:00, 191.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.271 acc_avg:87.336% acc:91.297% time:3s\n",
      "Testing loss:0.696 acc_avg:78.686% acc:84.968% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 49/100) Loss: 0.265 | Acc: 91.623% (9001/9824): 100%|██████████| 307/307 [00:02<00:00, 108.18it/s]\n",
      "( 49/100) Loss: 0.703 | Acc: 85.373% (2107/2468): 100%|██████████| 155/155 [00:00<00:00, 187.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.265 acc_avg:87.855% acc:91.623% time:3s\n",
      "Testing loss:0.703 acc_avg:79.765% acc:85.373% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 50/100) Loss: 0.258 | Acc: 91.572% (8996/9824): 100%|██████████| 307/307 [00:02<00:00, 107.12it/s]\n",
      "( 50/100) Loss: 0.715 | Acc: 85.292% (2105/2468): 100%|██████████| 155/155 [00:00<00:00, 194.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.258 acc_avg:88.032% acc:91.572% time:3s\n",
      "Testing loss:0.715 acc_avg:79.202% acc:85.292% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 51/100) Loss: 0.252 | Acc: 91.663% (9005/9824): 100%|██████████| 307/307 [00:02<00:00, 107.18it/s]\n",
      "( 51/100) Loss: 0.748 | Acc: 84.724% (2091/2468): 100%|██████████| 155/155 [00:00<00:00, 188.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.252 acc_avg:87.873% acc:91.663% time:3s\n",
      "Testing loss:0.748 acc_avg:78.966% acc:84.724% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 52/100) Loss: 0.252 | Acc: 91.755% (9014/9824): 100%|██████████| 307/307 [00:02<00:00, 107.09it/s]\n",
      "( 52/100) Loss: 0.746 | Acc: 85.292% (2105/2468): 100%|██████████| 155/155 [00:00<00:00, 190.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.252 acc_avg:87.665% acc:91.755% time:3s\n",
      "Testing loss:0.746 acc_avg:80.758% acc:85.292% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 53/100) Loss: 0.249 | Acc: 92.040% (9042/9824): 100%|██████████| 307/307 [00:02<00:00, 106.90it/s]\n",
      "( 53/100) Loss: 0.728 | Acc: 85.454% (2109/2468): 100%|██████████| 155/155 [00:00<00:00, 189.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.249 acc_avg:88.461% acc:92.04% time:3s\n",
      "Testing loss:0.728 acc_avg:79.644% acc:85.454% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 54/100) Loss: 0.238 | Acc: 92.213% (9059/9824): 100%|██████████| 307/307 [00:02<00:00, 106.63it/s]\n",
      "( 54/100) Loss: 0.764 | Acc: 84.887% (2095/2468): 100%|██████████| 155/155 [00:00<00:00, 190.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.238 acc_avg:88.591% acc:92.213% time:3s\n",
      "Testing loss:0.764 acc_avg:79.091% acc:84.887% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 55/100) Loss: 0.232 | Acc: 92.620% (9099/9824): 100%|██████████| 307/307 [00:02<00:00, 108.33it/s]\n",
      "( 55/100) Loss: 0.812 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 189.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.232 acc_avg:89.114% acc:92.62% time:3s\n",
      "Testing loss:0.812 acc_avg:80.049% acc:85.332% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 56/100) Loss: 0.235 | Acc: 92.366% (9074/9824): 100%|██████████| 307/307 [00:02<00:00, 108.95it/s]\n",
      "( 56/100) Loss: 0.753 | Acc: 85.292% (2105/2468): 100%|██████████| 155/155 [00:00<00:00, 192.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.235 acc_avg:88.447% acc:92.366% time:3s\n",
      "Testing loss:0.753 acc_avg:79.765% acc:85.292% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 57/100) Loss: 0.220 | Acc: 92.702% (9107/9824): 100%|██████████| 307/307 [00:02<00:00, 108.37it/s]\n",
      "( 57/100) Loss: 0.749 | Acc: 85.575% (2112/2468): 100%|██████████| 155/155 [00:00<00:00, 192.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.22 acc_avg:89.394% acc:92.702% time:3s\n",
      "Testing loss:0.749 acc_avg:80.266% acc:85.575% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 58/100) Loss: 0.216 | Acc: 92.793% (9116/9824): 100%|██████████| 307/307 [00:02<00:00, 106.85it/s]\n",
      "( 58/100) Loss: 0.781 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:00<00:00, 190.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.216 acc_avg:89.211% acc:92.793% time:3s\n",
      "Testing loss:0.781 acc_avg:79.403% acc:85.211% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 59/100) Loss: 0.208 | Acc: 92.997% (9136/9824): 100%|██████████| 307/307 [00:02<00:00, 105.92it/s]\n",
      "( 59/100) Loss: 0.788 | Acc: 85.373% (2107/2468): 100%|██████████| 155/155 [00:00<00:00, 192.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.208 acc_avg:89.561% acc:92.997% time:3s\n",
      "Testing loss:0.788 acc_avg:79.995% acc:85.373% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 60/100) Loss: 0.197 | Acc: 93.424% (9178/9824): 100%|██████████| 307/307 [00:02<00:00, 108.29it/s]\n",
      "( 60/100) Loss: 0.793 | Acc: 85.656% (2114/2468): 100%|██████████| 155/155 [00:00<00:00, 189.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.197 acc_avg:90.294% acc:93.424% time:3s\n",
      "Testing loss:0.793 acc_avg:80.074% acc:85.656% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 61/100) Loss: 0.193 | Acc: 93.679% (9203/9824): 100%|██████████| 307/307 [00:02<00:00, 107.43it/s]\n",
      "( 61/100) Loss: 0.807 | Acc: 85.008% (2098/2468): 100%|██████████| 155/155 [00:00<00:00, 182.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.193 acc_avg:90.797% acc:93.679% time:3s\n",
      "Testing loss:0.807 acc_avg:79.366% acc:85.008% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 62/100) Loss: 0.197 | Acc: 93.628% (9198/9824): 100%|██████████| 307/307 [00:02<00:00, 108.01it/s]\n",
      "( 62/100) Loss: 0.846 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 192.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.197 acc_avg:90.639% acc:93.628% time:3s\n",
      "Testing loss:0.846 acc_avg:80.019% acc:85.332% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 63/100) Loss: 0.184 | Acc: 93.923% (9227/9824): 100%|██████████| 307/307 [00:02<00:00, 107.37it/s]\n",
      "( 63/100) Loss: 0.818 | Acc: 84.968% (2097/2468): 100%|██████████| 155/155 [00:00<00:00, 187.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.184 acc_avg:91.003% acc:93.923% time:3s\n",
      "Testing loss:0.818 acc_avg:79.107% acc:84.968% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 64/100) Loss: 0.185 | Acc: 93.730% (9208/9824): 100%|██████████| 307/307 [00:02<00:00, 108.33it/s]\n",
      "( 64/100) Loss: 0.805 | Acc: 85.535% (2111/2468): 100%|██████████| 155/155 [00:00<00:00, 185.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.185 acc_avg:90.632% acc:93.73% time:3s\n",
      "Testing loss:0.805 acc_avg:79.548% acc:85.535% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 65/100) Loss: 0.186 | Acc: 94.076% (9242/9824): 100%|██████████| 307/307 [00:02<00:00, 106.62it/s]\n",
      "( 65/100) Loss: 0.826 | Acc: 85.778% (2117/2468): 100%|██████████| 155/155 [00:00<00:00, 185.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.186 acc_avg:91.255% acc:94.076% time:3s\n",
      "Testing loss:0.826 acc_avg:80.557% acc:85.778% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 66/100) Loss: 0.187 | Acc: 93.964% (9231/9824): 100%|██████████| 307/307 [00:02<00:00, 106.73it/s]\n",
      "( 66/100) Loss: 0.799 | Acc: 85.535% (2111/2468): 100%|██████████| 155/155 [00:00<00:00, 193.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.187 acc_avg:90.631% acc:93.964% time:3s\n",
      "Testing loss:0.799 acc_avg:80.036% acc:85.535% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 67/100) Loss: 0.172 | Acc: 94.239% (9258/9824): 100%|██████████| 307/307 [00:02<00:00, 107.85it/s]\n",
      "( 67/100) Loss: 0.803 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 187.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.172 acc_avg:91.603% acc:94.239% time:3s\n",
      "Testing loss:0.803 acc_avg:80.124% acc:85.332% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 68/100) Loss: 0.185 | Acc: 94.055% (9240/9824): 100%|██████████| 307/307 [00:02<00:00, 107.32it/s]\n",
      "( 68/100) Loss: 0.803 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 181.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.185 acc_avg:91.206% acc:94.055% time:3s\n",
      "Testing loss:0.803 acc_avg:80.074% acc:85.332% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 69/100) Loss: 0.161 | Acc: 94.473% (9281/9824): 100%|██████████| 307/307 [00:02<00:00, 107.62it/s]\n",
      "( 69/100) Loss: 0.815 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:00<00:00, 186.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.161 acc_avg:91.686% acc:94.473% time:3s\n",
      "Testing loss:0.815 acc_avg:79.303% acc:85.494% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 70/100) Loss: 0.157 | Acc: 94.391% (9273/9824): 100%|██████████| 307/307 [00:02<00:00, 107.39it/s]\n",
      "( 70/100) Loss: 0.836 | Acc: 85.697% (2115/2468): 100%|██████████| 155/155 [00:00<00:00, 192.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.157 acc_avg:91.498% acc:94.391% time:3s\n",
      "Testing loss:0.836 acc_avg:80.037% acc:85.697% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 71/100) Loss: 0.166 | Acc: 94.452% (9279/9824): 100%|██████████| 307/307 [00:02<00:00, 108.51it/s]\n",
      "( 71/100) Loss: 0.823 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:00<00:00, 194.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.166 acc_avg:91.648% acc:94.452% time:3s\n",
      "Testing loss:0.823 acc_avg:80.278% acc:85.494% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 72/100) Loss: 0.155 | Acc: 94.860% (9319/9824): 100%|██████████| 307/307 [00:02<00:00, 106.17it/s]\n",
      "( 72/100) Loss: 0.858 | Acc: 85.778% (2117/2468): 100%|██████████| 155/155 [00:00<00:00, 189.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.155 acc_avg:92.077% acc:94.86% time:3s\n",
      "Testing loss:0.858 acc_avg:80.132% acc:85.778% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 73/100) Loss: 0.149 | Acc: 94.819% (9315/9824): 100%|██████████| 307/307 [00:02<00:00, 108.97it/s]\n",
      "( 73/100) Loss: 0.885 | Acc: 84.968% (2097/2468): 100%|██████████| 155/155 [00:00<00:00, 186.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.149 acc_avg:92.501% acc:94.819% time:3s\n",
      "Testing loss:0.885 acc_avg:79.853% acc:84.968% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 74/100) Loss: 0.156 | Acc: 94.809% (9314/9824): 100%|██████████| 307/307 [00:02<00:00, 108.71it/s]\n",
      "( 74/100) Loss: 0.886 | Acc: 84.765% (2092/2468): 100%|██████████| 155/155 [00:00<00:00, 186.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.156 acc_avg:92.339% acc:94.809% time:3s\n",
      "Testing loss:0.886 acc_avg:79.224% acc:84.765% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 75/100) Loss: 0.147 | Acc: 94.748% (9308/9824): 100%|██████████| 307/307 [00:02<00:00, 108.35it/s]\n",
      "( 75/100) Loss: 0.853 | Acc: 85.292% (2105/2468): 100%|██████████| 155/155 [00:00<00:00, 189.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.147 acc_avg:92.262% acc:94.748% time:3s\n",
      "Testing loss:0.853 acc_avg:80.094% acc:85.292% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 76/100) Loss: 0.139 | Acc: 95.063% (9339/9824): 100%|██████████| 307/307 [00:02<00:00, 106.36it/s]\n",
      "( 76/100) Loss: 0.880 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:00<00:00, 189.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.139 acc_avg:92.448% acc:95.063% time:3s\n",
      "Testing loss:0.88 acc_avg:80.495% acc:85.494% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 77/100) Loss: 0.142 | Acc: 95.134% (9346/9824): 100%|██████████| 307/307 [00:02<00:00, 106.54it/s]\n",
      "( 77/100) Loss: 0.878 | Acc: 85.292% (2105/2468): 100%|██████████| 155/155 [00:00<00:00, 188.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.142 acc_avg:92.829% acc:95.134% time:3s\n",
      "Testing loss:0.878 acc_avg:80.098% acc:85.292% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 78/100) Loss: 0.143 | Acc: 95.307% (9363/9824): 100%|██████████| 307/307 [00:02<00:00, 108.09it/s]\n",
      "( 78/100) Loss: 0.871 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:00<00:00, 190.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.143 acc_avg:93.02% acc:95.307% time:3s\n",
      "Testing loss:0.871 acc_avg:80.107% acc:85.332% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 79/100) Loss: 0.140 | Acc: 95.328% (9365/9824): 100%|██████████| 307/307 [00:02<00:00, 108.64it/s]\n",
      "( 79/100) Loss: 0.876 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:00<00:00, 188.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.14 acc_avg:92.977% acc:95.328% time:3s\n",
      "Testing loss:0.876 acc_avg:79.728% acc:85.211% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 80/100) Loss: 0.139 | Acc: 95.165% (9349/9824): 100%|██████████| 307/307 [00:02<00:00, 106.62it/s]\n",
      "( 80/100) Loss: 0.852 | Acc: 85.251% (2104/2468): 100%|██████████| 155/155 [00:00<00:00, 197.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.139 acc_avg:92.831% acc:95.165% time:3s\n",
      "Testing loss:0.852 acc_avg:80.049% acc:85.251% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 81/100) Loss: 0.131 | Acc: 95.257% (9358/9824): 100%|██████████| 307/307 [00:02<00:00, 107.71it/s]\n",
      "( 81/100) Loss: 0.884 | Acc: 84.481% (2085/2468): 100%|██████████| 155/155 [00:00<00:00, 190.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.131 acc_avg:92.881% acc:95.257% time:3s\n",
      "Testing loss:0.884 acc_avg:79.462% acc:84.481% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 82/100) Loss: 0.121 | Acc: 95.867% (9418/9824): 100%|██████████| 307/307 [00:02<00:00, 107.07it/s]\n",
      "( 82/100) Loss: 0.841 | Acc: 85.575% (2112/2468): 100%|██████████| 155/155 [00:00<00:00, 186.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.121 acc_avg:93.991% acc:95.867% time:3s\n",
      "Testing loss:0.841 acc_avg:80.361% acc:85.575% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 83/100) Loss: 0.124 | Acc: 95.837% (9415/9824): 100%|██████████| 307/307 [00:02<00:00, 108.25it/s]\n",
      "( 83/100) Loss: 0.896 | Acc: 85.170% (2102/2468): 100%|██████████| 155/155 [00:00<00:00, 188.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.124 acc_avg:93.988% acc:95.837% time:3s\n",
      "Testing loss:0.896 acc_avg:79.912% acc:85.17% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 84/100) Loss: 0.125 | Acc: 95.542% (9386/9824): 100%|██████████| 307/307 [00:02<00:00, 107.69it/s]\n",
      "( 84/100) Loss: 0.923 | Acc: 85.089% (2100/2468): 100%|██████████| 155/155 [00:00<00:00, 189.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.125 acc_avg:93.381% acc:95.542% time:3s\n",
      "Testing loss:0.923 acc_avg:79.933% acc:85.089% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 85/100) Loss: 0.128 | Acc: 95.501% (9382/9824): 100%|██████████| 307/307 [00:02<00:00, 108.10it/s]\n",
      "( 85/100) Loss: 0.883 | Acc: 85.818% (2118/2468): 100%|██████████| 155/155 [00:00<00:00, 187.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.128 acc_avg:93.436% acc:95.501% time:3s\n",
      "Testing loss:0.883 acc_avg:80.587% acc:85.818% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 86/100) Loss: 0.118 | Acc: 95.745% (9406/9824): 100%|██████████| 307/307 [00:02<00:00, 106.82it/s]\n",
      "( 86/100) Loss: 0.909 | Acc: 85.535% (2111/2468): 100%|██████████| 155/155 [00:00<00:00, 184.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.118 acc_avg:93.719% acc:95.745% time:3s\n",
      "Testing loss:0.909 acc_avg:80.332% acc:85.535% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 87/100) Loss: 0.120 | Acc: 95.827% (9414/9824): 100%|██████████| 307/307 [00:02<00:00, 107.16it/s]\n",
      "( 87/100) Loss: 0.917 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:00<00:00, 193.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.12 acc_avg:93.551% acc:95.827% time:3s\n",
      "Testing loss:0.917 acc_avg:80.24% acc:85.211% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 88/100) Loss: 0.121 | Acc: 95.776% (9409/9824): 100%|██████████| 307/307 [00:02<00:00, 107.13it/s]\n",
      "( 88/100) Loss: 0.901 | Acc: 85.697% (2115/2468): 100%|██████████| 155/155 [00:00<00:00, 187.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.121 acc_avg:93.592% acc:95.776% time:3s\n",
      "Testing loss:0.901 acc_avg:80.34% acc:85.697% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 89/100) Loss: 0.115 | Acc: 95.939% (9425/9824): 100%|██████████| 307/307 [00:02<00:00, 107.03it/s]\n",
      "( 89/100) Loss: 0.912 | Acc: 85.616% (2113/2468): 100%|██████████| 155/155 [00:00<00:00, 186.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.115 acc_avg:93.898% acc:95.939% time:3s\n",
      "Testing loss:0.912 acc_avg:80.286% acc:85.616% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 90/100) Loss: 0.115 | Acc: 96.071% (9438/9824): 100%|██████████| 307/307 [00:02<00:00, 105.83it/s]\n",
      "( 90/100) Loss: 0.928 | Acc: 85.373% (2107/2468): 100%|██████████| 155/155 [00:00<00:00, 186.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.115 acc_avg:94.11% acc:96.071% time:3s\n",
      "Testing loss:0.928 acc_avg:80.294% acc:85.373% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 91/100) Loss: 0.118 | Acc: 95.928% (9424/9824): 100%|██████████| 307/307 [00:02<00:00, 107.98it/s]\n",
      "( 91/100) Loss: 0.911 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:00<00:00, 190.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.118 acc_avg:93.791% acc:95.928% time:3s\n",
      "Testing loss:0.911 acc_avg:79.828% acc:85.211% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 92/100) Loss: 0.113 | Acc: 95.745% (9406/9824): 100%|██████████| 307/307 [00:02<00:00, 107.73it/s]\n",
      "( 92/100) Loss: 0.918 | Acc: 84.724% (2091/2468): 100%|██████████| 155/155 [00:00<00:00, 187.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.113 acc_avg:93.557% acc:95.745% time:3s\n",
      "Testing loss:0.918 acc_avg:79.308% acc:84.724% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 93/100) Loss: 0.114 | Acc: 96.132% (9444/9824): 100%|██████████| 307/307 [00:02<00:00, 107.52it/s]\n",
      "( 93/100) Loss: 0.904 | Acc: 85.373% (2107/2468): 100%|██████████| 155/155 [00:00<00:00, 185.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.114 acc_avg:94.359% acc:96.132% time:3s\n",
      "Testing loss:0.904 acc_avg:79.64% acc:85.373% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 94/100) Loss: 0.116 | Acc: 96.173% (9448/9824): 100%|██████████| 307/307 [00:02<00:00, 108.33it/s]\n",
      "( 94/100) Loss: 0.909 | Acc: 84.846% (2094/2468): 100%|██████████| 155/155 [00:00<00:00, 187.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.116 acc_avg:94.421% acc:96.173% time:3s\n",
      "Testing loss:0.909 acc_avg:79.516% acc:84.846% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 95/100) Loss: 0.107 | Acc: 96.264% (9457/9824): 100%|██████████| 307/307 [00:02<00:00, 108.49it/s]\n",
      "( 95/100) Loss: 0.934 | Acc: 85.251% (2104/2468): 100%|██████████| 155/155 [00:00<00:00, 190.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.107 acc_avg:94.08% acc:96.264% time:3s\n",
      "Testing loss:0.934 acc_avg:80.049% acc:85.251% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 96/100) Loss: 0.120 | Acc: 95.765% (9408/9824): 100%|██████████| 307/307 [00:02<00:00, 108.34it/s]\n",
      "( 96/100) Loss: 0.910 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:00<00:00, 190.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.12 acc_avg:93.802% acc:95.765% time:3s\n",
      "Testing loss:0.91 acc_avg:79.732% acc:85.049% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 97/100) Loss: 0.105 | Acc: 96.386% (9469/9824): 100%|██████████| 307/307 [00:02<00:00, 107.45it/s]\n",
      "( 97/100) Loss: 0.921 | Acc: 85.535% (2111/2468): 100%|██████████| 155/155 [00:00<00:00, 191.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.105 acc_avg:94.497% acc:96.386% time:3s\n",
      "Testing loss:0.921 acc_avg:80.853% acc:85.535% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 98/100) Loss: 0.110 | Acc: 96.000% (9431/9824): 100%|██████████| 307/307 [00:02<00:00, 108.11it/s]\n",
      "( 98/100) Loss: 0.905 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:00<00:00, 193.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.11 acc_avg:94.213% acc:96.0% time:3s\n",
      "Testing loss:0.905 acc_avg:80.228% acc:85.494% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 99/100) Loss: 0.107 | Acc: 96.183% (9449/9824): 100%|██████████| 307/307 [00:02<00:00, 108.67it/s]\n",
      "( 99/100) Loss: 0.914 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:00<00:00, 192.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.107 acc_avg:94.025% acc:96.183% time:3s\n",
      "Testing loss:0.914 acc_avg:80.07% acc:85.049% time:1s [best test acc: 85.9%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training (Taken from PointMLP-PyTorch)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 100 # Number of epochs\n",
    "eval_every = 10 # Evaluate every 'eval_every' epochs\n",
    "max_lr = 0.1 # Maximum learning rate\n",
    "min_lr = 0.005 # Minimum learning rate\n",
    "\n",
    "best_test_acc = 0.  # best test accuracy\n",
    "best_train_acc = 0.\n",
    "best_test_acc_avg = 0.\n",
    "best_train_acc_avg = 0.\n",
    "best_test_loss = float(\"inf\")\n",
    "best_train_loss = float(\"inf\")\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Model\n",
    "net = MomentNet(3, [512, 256, 128], 40, degree = 20, poly_type = 'chebyshev').to(device)\n",
    "print(net.num_poly)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=max_lr, momentum=0.9, weight_decay=0)\n",
    "scheduler = CosineAnnealingLR(optimizer, num_epochs, eta_min=min_lr, last_epoch=start_epoch - 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_out = train(net, train_loader, optimizer, criterion, epoch, num_epochs, device)  # {\"loss\", \"acc\", \"acc_avg\", \"time\"}\n",
    "    test_out = validate(net, test_loader, criterion, epoch, num_epochs, device)\n",
    "    scheduler.step()\n",
    "    if test_out[\"acc\"] > best_test_acc:\n",
    "        best_test_acc = test_out[\"acc\"]\n",
    "        is_best = True\n",
    "    else:\n",
    "        is_best = False\n",
    "\n",
    "    best_test_acc = test_out[\"acc\"] if (test_out[\"acc\"] > best_test_acc) else best_test_acc\n",
    "    best_train_acc = train_out[\"acc\"] if (train_out[\"acc\"] > best_train_acc) else best_train_acc\n",
    "    best_test_acc_avg = test_out[\"acc_avg\"] if (test_out[\"acc_avg\"] > best_test_acc_avg) else best_test_acc_avg\n",
    "    best_train_acc_avg = train_out[\"acc_avg\"] if (train_out[\"acc_avg\"] > best_train_acc_avg) else best_train_acc_avg\n",
    "    best_test_loss = test_out[\"loss\"] if (test_out[\"loss\"] < best_test_loss) else best_test_loss\n",
    "    best_train_loss = train_out[\"loss\"] if (train_out[\"loss\"] < best_train_loss) else best_train_loss\n",
    "    \n",
    "    print(\n",
    "        f\"Training loss:{train_out['loss']} acc_avg:{train_out['acc_avg']}% acc:{train_out['acc']}% time:{train_out['time']}s\")\n",
    "    print(\n",
    "        f\"Testing loss:{test_out['loss']} acc_avg:{test_out['acc_avg']}% \"\n",
    "        f\"acc:{test_out['acc']}% time:{test_out['time']}s [best test acc: {best_test_acc}%]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
