{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations_with_replacement\n",
    "from numpy.linalg import qr, norm\n",
    "import open3d as o3d\n",
    "import datetime\n",
    "from utils import Logger, mkdir_p, progress_bar, save_model, save_args, cal_loss\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from data import ModelNet40\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def generate_monomials_sequences_batch(X, degree):\n",
    "    \"\"\"\n",
    "    Generate monomials up to a given degree for d-dimensional points.\n",
    "\n",
    "    Parameters:\n",
    "        X (ndarray): An array of shape (B, N, d) representing the point cloud.\n",
    "        degree (int): The maximum degree of the monomials.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Monomial matrix of shape (B, N, M).\n",
    "    \"\"\"\n",
    "    B, N, d = X.shape\n",
    "    device = X.device\n",
    "    exponents = []\n",
    "    # Generate all multi-indices with total degree up to 'degree'\n",
    "    for total_degree in range(degree + 1):\n",
    "        for exps in combinations_with_replacement(range(d), total_degree):\n",
    "            exp = torch.zeros(d, dtype=int, device=device)\n",
    "            for var in exps:\n",
    "                exp[var] += 1\n",
    "            exponents.append(exp)\n",
    "    M = len(exponents)\n",
    "    # print(f'Number of monomials: {M}') # Number of polynomials: n1 + n2 + ... + n_d = degree; degree + d choose d; d number of dividers for an array in space R^d.\n",
    "    monomials = torch.ones(B, N, M, device=device)\n",
    "    for i, exp in enumerate(exponents):\n",
    "        monomials[:, :, i] = torch.prod(X ** exp, axis=2) # x1^exp1 * x2^exp2 * ... * xd^expd. e.g. x1^2 * x2^3 * x3^1 \\in R^3\n",
    "    return monomials.sum(dim=1) / N # (B, N, M) -> (B, M)\n",
    "\n",
    "def generate_chebyshev_polynomials_sequence_batch(X, degree):\n",
    "    \"\"\"\n",
    "    Generate Chebyshev polynomials up to a given degree for d-dimensional points.\n",
    "\n",
    "    Parameters:\n",
    "        X (ndarray): An array of shape (B, N, d) representing the d-dimensional point cloud.\n",
    "        degree (int): The maximum degree of the polynomials.\n",
    "\n",
    "    Returns:\n",
    "        ndarray: Chebyshev polynomial matrix of shape (B, N, M).\n",
    "    \"\"\"\n",
    "    B, N, d = X.shape\n",
    "    device = X.device\n",
    "    exponents = []\n",
    "    # Generate all multi-indices with total degree up to 'degree'\n",
    "    for total_degree in range(degree + 1):\n",
    "        for exps in combinations_with_replacement(range(d), total_degree):\n",
    "            exp = torch.zeros(d, dtype=int, device=device)\n",
    "            for var in exps:\n",
    "                exp[var] += 1\n",
    "            exponents.append(exp)\n",
    "    M = len(exponents)\n",
    "    exponent = torch.tensor(exponents, device=device) # (M, d)\n",
    "    # print(f'Number of Chebyshev polynomials: {M}') # Number of polynomials: n1 + n2 + ... + n_d = degree; degree + d choose d; d number of dividers for an array in space R^d.\n",
    "    # cheby_polynomials = torch.ones(B, N, M, device=device)\n",
    "    # for i, exp in enumerate(exponents):\n",
    "    #     cheby_polynomials[:, :, i] = torch.prod(torch.cos(exp * torch.acos(X)), axis=2)\n",
    "    cheby_polynomials = torch.cos(exponent.unsqueeze(0) * torch.acos(X)) # (B, N, M)\n",
    "    cheby_polynomials = cheby_polynomials.prod(dim=2) # (B, N)\n",
    "    \n",
    "    \n",
    "    return cheby_polynomials.sum(dim=1) / N # (B, N, M) -> (B, M)\n",
    "\n",
    "def poly_seq_batch(X, degree, poly_type='monomials'):\n",
    "    if poly_type == 'monomials':\n",
    "        return generate_monomials_sequences_batch(X, degree)\n",
    "    elif poly_type == 'chebyshev':\n",
    "        return generate_chebyshev_polynomials_sequence_batch(X, degree)\n",
    "    else:\n",
    "        raise ValueError('Invalid polynomial type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use ModelNet40 as our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 2048\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(ModelNet40(partition='train', \n",
    "                                        num_points=num_points,\n",
    "                                        aug=True), \n",
    "                            num_workers=8,\n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=True, \n",
    "                            drop_last=True)\n",
    "test_loader = DataLoader(ModelNet40(partition='test', \n",
    "                                        num_points=num_points,\n",
    "                                        aug=False), \n",
    "                            num_workers=8,\n",
    "                            batch_size=batch_size // 2, \n",
    "                            shuffle=False, \n",
    "                            drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "class MLPClaasifier(nn.Module):\n",
    "    def __init__(self, dim_in, layer_dims, dim_out, dropout=0.5):\n",
    "        super(MLPClaasifier, self).__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(nn.Sequential(\n",
    "            nn.Linear(dim_in, layer_dims[0]),\n",
    "            nn.BatchNorm1d(layer_dims[0]),\n",
    "            nn.ReLU(),\n",
    "        ))\n",
    "        for i in range(1, len(layer_dims)):\n",
    "            self.convs.append(nn.Sequential(\n",
    "                nn.Linear(layer_dims[i-1], layer_dims[i]),\n",
    "                nn.BatchNorm1d(layer_dims[i]),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.ReLU(),\n",
    "            ))\n",
    "        self.convs.append(nn.Linear(layer_dims[-1], dim_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, conv in enumerate(self.convs[:-1]):\n",
    "            x = conv(x)\n",
    "        x = self.convs[-1](x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(32, 100) # 32 samples, 100 features\n",
    "model = MLPClaasifier(100, [128, 64], 40)\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 40])\n"
     ]
    }
   ],
   "source": [
    "class MomentNet(nn.Module):\n",
    "    def __init__(self, dim_in, layer_dims, dim_out, degree, poly_type='monomials'):\n",
    "        super(MomentNet, self).__init__()\n",
    "        self.poly_type = poly_type\n",
    "        self.degree = degree\n",
    "        self.num_poly = math.comb(degree + dim_in, dim_in)\n",
    "        self.mlp = MLPClaasifier(self.num_poly, layer_dims, dim_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = poly_seq_batch(x, self.degree, self.poly_type)\n",
    "        x = self.mlp(x)\n",
    "        return x\n",
    "    \n",
    "x = torch.randn(32, 2048, 3) # 32 samples, 2048 points, 3 features\n",
    "model = MomentNet(3, [128, 64], 40, 2, 'monomials')\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, optimizer, criterion, epoch, num_epochs, device):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    train_pred = []\n",
    "    train_true = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    pbar = tqdm(enumerate(trainloader), total=len(trainloader))\n",
    "    for batch_idx, (data, label) in pbar:\n",
    "        data, label = data.to(device), label.to(device).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        logits = net(data)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        preds = logits.max(dim=1)[1]\n",
    "\n",
    "        train_true.append(label.cpu().numpy())\n",
    "        train_pred.append(preds.detach().cpu().numpy())\n",
    "\n",
    "        total += label.size(0)\n",
    "        correct += preds.eq(label).sum().item()\n",
    "\n",
    "        pbar.set_description(f'({epoch:3d}/{num_epochs:3d}) Loss: {train_loss / (batch_idx + 1):.3f} | Acc: {100. * correct / total:.3f}% ({correct}/{total})')\n",
    "        # progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        #              % (train_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    train_true = np.concatenate(train_true)\n",
    "    train_pred = np.concatenate(train_pred)\n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (train_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(train_true, train_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(train_true, train_pred))),\n",
    "        \"time\": time_cost\n",
    "    }\n",
    "\n",
    "\n",
    "def validate(net, testloader, criterion, epoch, num_epochs, device):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    time_cost = datetime.datetime.now()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(enumerate(testloader), total=len(testloader))\n",
    "        for batch_idx, (data, label) in pbar:\n",
    "            data, label = data.to(device), label.to(device).squeeze()\n",
    "            logits = net(data)\n",
    "            loss = criterion(logits, label)\n",
    "            test_loss += loss.item()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            test_true.append(label.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "            total += label.size(0)\n",
    "            correct += preds.eq(label).sum().item()\n",
    "            pbar.set_description(f'({epoch:3d}/{num_epochs:3d}) Loss: {test_loss / (batch_idx + 1):.3f} | Acc: {100. * correct / total:.3f}% ({correct}/{total})')\n",
    "            # progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss / (batch_idx + 1), 100. * correct / total, correct, total))\n",
    "\n",
    "    time_cost = int((datetime.datetime.now() - time_cost).total_seconds())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    return {\n",
    "        \"loss\": float(\"%.3f\" % (test_loss / (batch_idx + 1))),\n",
    "        \"acc\": float(\"%.3f\" % (100. * metrics.accuracy_score(test_true, test_pred))),\n",
    "        \"acc_avg\": float(\"%.3f\" % (100. * metrics.balanced_accuracy_score(test_true, test_pred))),\n",
    "        \"time\": time_cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(  0/100) Loss: 1.638 | Acc: 56.158% (5517/9824): 100%|██████████| 307/307 [00:23<00:00, 13.05it/s]\n",
      "(  0/100) Loss: 0.977 | Acc: 72.366% (1786/2468): 100%|██████████| 155/155 [00:11<00:00, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.638 acc_avg:39.928% acc:56.158% time:23s\n",
      "Testing loss:0.977 acc_avg:62.128% acc:72.366% time:11s [best test acc: 72.366%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  1/100) Loss: 1.139 | Acc: 67.590% (6640/9824): 100%|██████████| 307/307 [00:22<00:00, 13.55it/s]\n",
      "(  1/100) Loss: 0.871 | Acc: 74.757% (1845/2468): 100%|██████████| 155/155 [00:11<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:1.139 acc_avg:54.65% acc:67.59% time:23s\n",
      "Testing loss:0.871 acc_avg:64.159% acc:74.757% time:11s [best test acc: 74.757%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  2/100) Loss: 0.993 | Acc: 71.274% (7002/9824): 100%|██████████| 307/307 [00:22<00:00, 13.37it/s]\n",
      "(  2/100) Loss: 0.903 | Acc: 73.906% (1824/2468): 100%|██████████| 155/155 [00:11<00:00, 13.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.993 acc_avg:59.657% acc:71.274% time:23s\n",
      "Testing loss:0.903 acc_avg:66.659% acc:73.906% time:11s [best test acc: 74.757%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  3/100) Loss: 0.926 | Acc: 72.934% (7165/9824): 100%|██████████| 307/307 [00:22<00:00, 13.50it/s]\n",
      "(  3/100) Loss: 0.794 | Acc: 75.203% (1856/2468): 100%|██████████| 155/155 [00:11<00:00, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.926 acc_avg:61.656% acc:72.934% time:23s\n",
      "Testing loss:0.794 acc_avg:66.651% acc:75.203% time:11s [best test acc: 75.203%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  4/100) Loss: 0.878 | Acc: 74.827% (7351/9824): 100%|██████████| 307/307 [00:22<00:00, 13.36it/s]\n",
      "(  4/100) Loss: 0.707 | Acc: 79.538% (1963/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.878 acc_avg:64.348% acc:74.827% time:23s\n",
      "Testing loss:0.707 acc_avg:70.803% acc:79.538% time:11s [best test acc: 79.538%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  5/100) Loss: 0.822 | Acc: 75.855% (7452/9824): 100%|██████████| 307/307 [00:22<00:00, 13.53it/s]\n",
      "(  5/100) Loss: 0.706 | Acc: 80.105% (1977/2468): 100%|██████████| 155/155 [00:11<00:00, 13.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.822 acc_avg:65.623% acc:75.855% time:23s\n",
      "Testing loss:0.706 acc_avg:71.047% acc:80.105% time:11s [best test acc: 80.105%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  6/100) Loss: 0.792 | Acc: 76.232% (7489/9824): 100%|██████████| 307/307 [00:22<00:00, 13.50it/s]\n",
      "(  6/100) Loss: 0.675 | Acc: 79.660% (1966/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.792 acc_avg:66.182% acc:76.232% time:23s\n",
      "Testing loss:0.675 acc_avg:70.727% acc:79.66% time:11s [best test acc: 80.105%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  7/100) Loss: 0.758 | Acc: 77.402% (7604/9824): 100%|██████████| 307/307 [00:22<00:00, 13.56it/s]\n",
      "(  7/100) Loss: 0.694 | Acc: 80.429% (1985/2468): 100%|██████████| 155/155 [00:11<00:00, 13.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.758 acc_avg:68.117% acc:77.402% time:22s\n",
      "Testing loss:0.694 acc_avg:71.22% acc:80.429% time:11s [best test acc: 80.429%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  8/100) Loss: 0.738 | Acc: 78.003% (7663/9824): 100%|██████████| 307/307 [00:22<00:00, 13.41it/s]\n",
      "(  8/100) Loss: 0.723 | Acc: 78.728% (1943/2468): 100%|██████████| 155/155 [00:11<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.738 acc_avg:68.823% acc:78.003% time:23s\n",
      "Testing loss:0.723 acc_avg:69.922% acc:78.728% time:11s [best test acc: 80.429%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(  9/100) Loss: 0.717 | Acc: 78.532% (7715/9824): 100%|██████████| 307/307 [00:22<00:00, 13.47it/s]\n",
      "(  9/100) Loss: 0.652 | Acc: 80.754% (1993/2468): 100%|██████████| 155/155 [00:11<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.717 acc_avg:69.634% acc:78.532% time:23s\n",
      "Testing loss:0.652 acc_avg:73.284% acc:80.754% time:11s [best test acc: 80.754%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 10/100) Loss: 0.695 | Acc: 79.316% (7792/9824): 100%|██████████| 307/307 [00:22<00:00, 13.39it/s]\n",
      "( 10/100) Loss: 0.664 | Acc: 79.862% (1971/2468): 100%|██████████| 155/155 [00:11<00:00, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.695 acc_avg:70.482% acc:79.316% time:23s\n",
      "Testing loss:0.664 acc_avg:72.39% acc:79.862% time:11s [best test acc: 80.754%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 11/100) Loss: 0.687 | Acc: 79.499% (7810/9824): 100%|██████████| 307/307 [00:23<00:00, 13.09it/s]\n",
      "( 11/100) Loss: 0.667 | Acc: 81.037% (2000/2468): 100%|██████████| 155/155 [00:11<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.687 acc_avg:70.405% acc:79.499% time:23s\n",
      "Testing loss:0.667 acc_avg:73.844% acc:81.037% time:11s [best test acc: 81.037%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 12/100) Loss: 0.653 | Acc: 80.578% (7916/9824): 100%|██████████| 307/307 [00:22<00:00, 13.57it/s]\n",
      "( 12/100) Loss: 0.642 | Acc: 81.645% (2015/2468): 100%|██████████| 155/155 [00:11<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.653 acc_avg:72.223% acc:80.578% time:22s\n",
      "Testing loss:0.642 acc_avg:75.187% acc:81.645% time:11s [best test acc: 81.645%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 13/100) Loss: 0.652 | Acc: 80.334% (7892/9824): 100%|██████████| 307/307 [00:23<00:00, 13.15it/s]\n",
      "( 13/100) Loss: 0.649 | Acc: 81.280% (2006/2468): 100%|██████████| 155/155 [00:11<00:00, 13.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.652 acc_avg:71.981% acc:80.334% time:23s\n",
      "Testing loss:0.649 acc_avg:74.161% acc:81.28% time:12s [best test acc: 81.645%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 14/100) Loss: 0.626 | Acc: 81.077% (7965/9824): 100%|██████████| 307/307 [00:23<00:00, 13.30it/s]\n",
      "( 14/100) Loss: 0.604 | Acc: 81.969% (2023/2468): 100%|██████████| 155/155 [00:11<00:00, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.626 acc_avg:73.109% acc:81.077% time:23s\n",
      "Testing loss:0.604 acc_avg:74.808% acc:81.969% time:11s [best test acc: 81.969%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 15/100) Loss: 0.613 | Acc: 81.749% (8031/9824): 100%|██████████| 307/307 [00:23<00:00, 13.19it/s]\n",
      "( 15/100) Loss: 0.608 | Acc: 83.104% (2051/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.613 acc_avg:73.948% acc:81.749% time:23s\n",
      "Testing loss:0.608 acc_avg:76.322% acc:83.104% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 16/100) Loss: 0.603 | Acc: 81.586% (8015/9824): 100%|██████████| 307/307 [00:22<00:00, 13.42it/s]\n",
      "( 16/100) Loss: 0.615 | Acc: 81.645% (2015/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.603 acc_avg:73.958% acc:81.586% time:23s\n",
      "Testing loss:0.615 acc_avg:73.549% acc:81.645% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 17/100) Loss: 0.586 | Acc: 81.993% (8055/9824): 100%|██████████| 307/307 [00:22<00:00, 13.42it/s]\n",
      "( 17/100) Loss: 0.625 | Acc: 82.455% (2035/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.586 acc_avg:74.089% acc:81.993% time:23s\n",
      "Testing loss:0.625 acc_avg:75.095% acc:82.455% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 18/100) Loss: 0.585 | Acc: 82.024% (8058/9824): 100%|██████████| 307/307 [00:22<00:00, 13.39it/s]\n",
      "( 18/100) Loss: 0.662 | Acc: 81.524% (2012/2468): 100%|██████████| 155/155 [00:11<00:00, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.585 acc_avg:74.379% acc:82.024% time:23s\n",
      "Testing loss:0.662 acc_avg:74.507% acc:81.524% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 19/100) Loss: 0.577 | Acc: 82.563% (8111/9824): 100%|██████████| 307/307 [00:22<00:00, 13.39it/s]\n",
      "( 19/100) Loss: 0.583 | Acc: 82.658% (2040/2468): 100%|██████████| 155/155 [00:11<00:00, 13.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.577 acc_avg:75.225% acc:82.563% time:23s\n",
      "Testing loss:0.583 acc_avg:75.761% acc:82.658% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 20/100) Loss: 0.550 | Acc: 83.011% (8155/9824): 100%|██████████| 307/307 [00:22<00:00, 13.52it/s]\n",
      "( 20/100) Loss: 0.606 | Acc: 82.699% (2041/2468): 100%|██████████| 155/155 [00:11<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.55 acc_avg:75.64% acc:83.011% time:23s\n",
      "Testing loss:0.606 acc_avg:76.952% acc:82.699% time:11s [best test acc: 83.104%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 21/100) Loss: 0.541 | Acc: 83.052% (8159/9824): 100%|██████████| 307/307 [00:22<00:00, 13.43it/s]\n",
      "( 21/100) Loss: 0.602 | Acc: 83.590% (2063/2468): 100%|██████████| 155/155 [00:11<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.541 acc_avg:75.756% acc:83.052% time:23s\n",
      "Testing loss:0.602 acc_avg:76.854% acc:83.59% time:11s [best test acc: 83.59%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 22/100) Loss: 0.548 | Acc: 82.940% (8148/9824): 100%|██████████| 307/307 [00:22<00:00, 13.37it/s]\n",
      "( 22/100) Loss: 0.580 | Acc: 84.157% (2077/2468): 100%|██████████| 155/155 [00:11<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.548 acc_avg:75.112% acc:82.94% time:23s\n",
      "Testing loss:0.58 acc_avg:76.173% acc:84.157% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 23/100) Loss: 0.521 | Acc: 83.642% (8217/9824): 100%|██████████| 307/307 [00:23<00:00, 12.98it/s]\n",
      "( 23/100) Loss: 0.602 | Acc: 83.630% (2064/2468): 100%|██████████| 155/155 [00:11<00:00, 13.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.521 acc_avg:76.639% acc:83.642% time:24s\n",
      "Testing loss:0.602 acc_avg:76.669% acc:83.63% time:12s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 24/100) Loss: 0.524 | Acc: 84.080% (8260/9824): 100%|██████████| 307/307 [00:23<00:00, 12.88it/s]\n",
      "( 24/100) Loss: 0.603 | Acc: 84.076% (2075/2468): 100%|██████████| 155/155 [00:11<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.524 acc_avg:77.199% acc:84.08% time:24s\n",
      "Testing loss:0.603 acc_avg:77.137% acc:84.076% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 25/100) Loss: 0.507 | Acc: 84.497% (8301/9824): 100%|██████████| 307/307 [00:23<00:00, 13.04it/s]\n",
      "( 25/100) Loss: 0.581 | Acc: 83.874% (2070/2468): 100%|██████████| 155/155 [00:11<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.507 acc_avg:77.693% acc:84.497% time:23s\n",
      "Testing loss:0.581 acc_avg:77.144% acc:83.874% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 26/100) Loss: 0.502 | Acc: 83.988% (8251/9824): 100%|██████████| 307/307 [00:23<00:00, 12.91it/s]\n",
      "( 26/100) Loss: 0.614 | Acc: 83.752% (2067/2468): 100%|██████████| 155/155 [00:11<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.502 acc_avg:76.737% acc:83.988% time:24s\n",
      "Testing loss:0.614 acc_avg:78.74% acc:83.752% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 27/100) Loss: 0.490 | Acc: 84.579% (8309/9824): 100%|██████████| 307/307 [00:23<00:00, 13.03it/s]\n",
      "( 27/100) Loss: 0.606 | Acc: 83.874% (2070/2468): 100%|██████████| 155/155 [00:11<00:00, 13.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.49 acc_avg:77.756% acc:84.579% time:23s\n",
      "Testing loss:0.606 acc_avg:76.981% acc:83.874% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 28/100) Loss: 0.489 | Acc: 84.375% (8289/9824): 100%|██████████| 307/307 [00:23<00:00, 13.01it/s]\n",
      "( 28/100) Loss: 0.606 | Acc: 83.712% (2066/2468): 100%|██████████| 155/155 [00:11<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.489 acc_avg:77.644% acc:84.375% time:23s\n",
      "Testing loss:0.606 acc_avg:76.886% acc:83.712% time:12s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 29/100) Loss: 0.467 | Acc: 85.291% (8379/9824): 100%|██████████| 307/307 [00:23<00:00, 13.18it/s]\n",
      "( 29/100) Loss: 0.600 | Acc: 84.076% (2075/2468): 100%|██████████| 155/155 [00:11<00:00, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.467 acc_avg:79.19% acc:85.291% time:23s\n",
      "Testing loss:0.6 acc_avg:75.743% acc:84.076% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 30/100) Loss: 0.478 | Acc: 85.067% (8357/9824): 100%|██████████| 307/307 [00:23<00:00, 13.04it/s]\n",
      "( 30/100) Loss: 0.613 | Acc: 84.157% (2077/2468): 100%|██████████| 155/155 [00:11<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.478 acc_avg:78.784% acc:85.067% time:23s\n",
      "Testing loss:0.613 acc_avg:76.607% acc:84.157% time:11s [best test acc: 84.157%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 31/100) Loss: 0.461 | Acc: 85.118% (8362/9824): 100%|██████████| 307/307 [00:23<00:00, 12.91it/s]\n",
      "( 31/100) Loss: 0.555 | Acc: 85.170% (2102/2468): 100%|██████████| 155/155 [00:11<00:00, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.461 acc_avg:78.701% acc:85.118% time:24s\n",
      "Testing loss:0.555 acc_avg:77.04% acc:85.17% time:11s [best test acc: 85.17%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 32/100) Loss: 0.462 | Acc: 85.301% (8380/9824): 100%|██████████| 307/307 [00:23<00:00, 13.25it/s]\n",
      "( 32/100) Loss: 0.580 | Acc: 84.562% (2087/2468): 100%|██████████| 155/155 [00:11<00:00, 13.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.462 acc_avg:79.061% acc:85.301% time:23s\n",
      "Testing loss:0.58 acc_avg:77.507% acc:84.562% time:11s [best test acc: 85.17%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 33/100) Loss: 0.428 | Acc: 86.187% (8467/9824): 100%|██████████| 307/307 [00:22<00:00, 13.42it/s]\n",
      "( 33/100) Loss: 0.592 | Acc: 83.914% (2071/2468): 100%|██████████| 155/155 [00:11<00:00, 13.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.428 acc_avg:79.938% acc:86.187% time:23s\n",
      "Testing loss:0.592 acc_avg:77.758% acc:83.914% time:11s [best test acc: 85.17%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 34/100) Loss: 0.432 | Acc: 86.095% (8458/9824): 100%|██████████| 307/307 [00:22<00:00, 13.50it/s]\n",
      "( 34/100) Loss: 0.618 | Acc: 84.076% (2075/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.432 acc_avg:80.151% acc:86.095% time:23s\n",
      "Testing loss:0.618 acc_avg:78.816% acc:84.076% time:11s [best test acc: 85.17%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 35/100) Loss: 0.438 | Acc: 85.708% (8420/9824): 100%|██████████| 307/307 [00:22<00:00, 13.53it/s]\n",
      "( 35/100) Loss: 0.610 | Acc: 84.117% (2076/2468): 100%|██████████| 155/155 [00:11<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.438 acc_avg:79.46% acc:85.708% time:23s\n",
      "Testing loss:0.61 acc_avg:78.695% acc:84.117% time:11s [best test acc: 85.17%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 36/100) Loss: 0.416 | Acc: 86.747% (8522/9824): 100%|██████████| 307/307 [00:22<00:00, 13.57it/s]\n",
      "( 36/100) Loss: 0.576 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.416 acc_avg:81.019% acc:86.747% time:22s\n",
      "Testing loss:0.576 acc_avg:78.34% acc:85.211% time:11s [best test acc: 85.211%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 37/100) Loss: 0.417 | Acc: 86.126% (8461/9824): 100%|██████████| 307/307 [00:22<00:00, 13.51it/s]\n",
      "( 37/100) Loss: 0.634 | Acc: 83.874% (2070/2468): 100%|██████████| 155/155 [00:11<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.417 acc_avg:79.966% acc:86.126% time:23s\n",
      "Testing loss:0.634 acc_avg:78.54% acc:83.874% time:11s [best test acc: 85.211%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 38/100) Loss: 0.413 | Acc: 86.309% (8479/9824): 100%|██████████| 307/307 [00:22<00:00, 13.44it/s]\n",
      "( 38/100) Loss: 0.611 | Acc: 85.170% (2102/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.413 acc_avg:80.346% acc:86.309% time:23s\n",
      "Testing loss:0.611 acc_avg:78.506% acc:85.17% time:11s [best test acc: 85.211%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 39/100) Loss: 0.401 | Acc: 87.174% (8564/9824): 100%|██████████| 307/307 [00:22<00:00, 13.44it/s]\n",
      "( 39/100) Loss: 0.610 | Acc: 85.413% (2108/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.401 acc_avg:81.56% acc:87.174% time:23s\n",
      "Testing loss:0.61 acc_avg:79.42% acc:85.413% time:11s [best test acc: 85.413%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 40/100) Loss: 0.387 | Acc: 87.103% (8557/9824): 100%|██████████| 307/307 [00:22<00:00, 13.67it/s]\n",
      "( 40/100) Loss: 0.604 | Acc: 85.413% (2108/2468): 100%|██████████| 155/155 [00:11<00:00, 13.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.387 acc_avg:81.105% acc:87.103% time:22s\n",
      "Testing loss:0.604 acc_avg:79.348% acc:85.413% time:11s [best test acc: 85.413%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 41/100) Loss: 0.386 | Acc: 87.205% (8567/9824): 100%|██████████| 307/307 [00:22<00:00, 13.46it/s]\n",
      "( 41/100) Loss: 0.600 | Acc: 85.089% (2100/2468): 100%|██████████| 155/155 [00:11<00:00, 13.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.386 acc_avg:81.417% acc:87.205% time:23s\n",
      "Testing loss:0.6 acc_avg:79.598% acc:85.089% time:11s [best test acc: 85.413%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 42/100) Loss: 0.372 | Acc: 87.775% (8623/9824): 100%|██████████| 307/307 [00:22<00:00, 13.53it/s]\n",
      "( 42/100) Loss: 0.587 | Acc: 86.143% (2126/2468): 100%|██████████| 155/155 [00:11<00:00, 13.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.372 acc_avg:82.268% acc:87.775% time:23s\n",
      "Testing loss:0.587 acc_avg:79.131% acc:86.143% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 43/100) Loss: 0.369 | Acc: 87.846% (8630/9824): 100%|██████████| 307/307 [00:22<00:00, 13.68it/s]\n",
      "( 43/100) Loss: 0.614 | Acc: 84.562% (2087/2468): 100%|██████████| 155/155 [00:11<00:00, 13.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.369 acc_avg:82.526% acc:87.846% time:22s\n",
      "Testing loss:0.614 acc_avg:78.553% acc:84.562% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 44/100) Loss: 0.361 | Acc: 88.182% (8663/9824): 100%|██████████| 307/307 [00:22<00:00, 13.47it/s]\n",
      "( 44/100) Loss: 0.617 | Acc: 84.927% (2096/2468): 100%|██████████| 155/155 [00:11<00:00, 13.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.361 acc_avg:82.739% acc:88.182% time:23s\n",
      "Testing loss:0.617 acc_avg:78.924% acc:84.927% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 45/100) Loss: 0.372 | Acc: 87.693% (8615/9824): 100%|██████████| 307/307 [00:22<00:00, 13.35it/s]\n",
      "( 45/100) Loss: 0.595 | Acc: 85.413% (2108/2468): 100%|██████████| 155/155 [00:11<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.372 acc_avg:82.021% acc:87.693% time:23s\n",
      "Testing loss:0.595 acc_avg:79.469% acc:85.413% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 46/100) Loss: 0.365 | Acc: 88.009% (8646/9824): 100%|██████████| 307/307 [00:22<00:00, 13.38it/s]\n",
      "( 46/100) Loss: 0.642 | Acc: 84.441% (2084/2468): 100%|██████████| 155/155 [00:11<00:00, 13.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.365 acc_avg:82.842% acc:88.009% time:23s\n",
      "Testing loss:0.642 acc_avg:78.827% acc:84.441% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 47/100) Loss: 0.361 | Acc: 88.284% (8673/9824): 100%|██████████| 307/307 [00:22<00:00, 13.45it/s]\n",
      "( 47/100) Loss: 0.619 | Acc: 84.198% (2078/2468): 100%|██████████| 155/155 [00:11<00:00, 13.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.361 acc_avg:83.31% acc:88.284% time:23s\n",
      "Testing loss:0.619 acc_avg:78.749% acc:84.198% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 48/100) Loss: 0.361 | Acc: 87.989% (8644/9824): 100%|██████████| 307/307 [00:22<00:00, 13.47it/s]\n",
      "( 48/100) Loss: 0.612 | Acc: 84.765% (2092/2468): 100%|██████████| 155/155 [00:11<00:00, 13.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.361 acc_avg:82.297% acc:87.989% time:23s\n",
      "Testing loss:0.612 acc_avg:78.931% acc:84.765% time:11s [best test acc: 86.143%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 49/100) Loss: 0.342 | Acc: 88.701% (8714/9824): 100%|██████████| 307/307 [00:22<00:00, 13.57it/s]\n",
      "( 49/100) Loss: 0.607 | Acc: 86.264% (2129/2468): 100%|██████████| 155/155 [00:11<00:00, 13.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.342 acc_avg:83.365% acc:88.701% time:22s\n",
      "Testing loss:0.607 acc_avg:79.982% acc:86.264% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 50/100) Loss: 0.345 | Acc: 88.701% (8714/9824): 100%|██████████| 307/307 [00:22<00:00, 13.48it/s]\n",
      "( 50/100) Loss: 0.625 | Acc: 85.656% (2114/2468): 100%|██████████| 155/155 [00:11<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.345 acc_avg:83.721% acc:88.701% time:23s\n",
      "Testing loss:0.625 acc_avg:80.252% acc:85.656% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 51/100) Loss: 0.328 | Acc: 88.660% (8710/9824): 100%|██████████| 307/307 [00:22<00:00, 13.54it/s]\n",
      "( 51/100) Loss: 0.651 | Acc: 85.697% (2115/2468): 100%|██████████| 155/155 [00:11<00:00, 13.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.328 acc_avg:83.911% acc:88.66% time:23s\n",
      "Testing loss:0.651 acc_avg:79.702% acc:85.697% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 52/100) Loss: 0.329 | Acc: 88.701% (8714/9824): 100%|██████████| 307/307 [00:23<00:00, 13.26it/s]\n",
      "( 52/100) Loss: 0.645 | Acc: 84.765% (2092/2468): 100%|██████████| 155/155 [00:11<00:00, 13.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.329 acc_avg:83.72% acc:88.701% time:23s\n",
      "Testing loss:0.645 acc_avg:78.16% acc:84.765% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 53/100) Loss: 0.316 | Acc: 89.210% (8764/9824): 100%|██████████| 307/307 [00:23<00:00, 13.31it/s]\n",
      "( 53/100) Loss: 0.666 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:11<00:00, 13.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.316 acc_avg:84.577% acc:89.21% time:23s\n",
      "Testing loss:0.666 acc_avg:79.766% acc:85.332% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 54/100) Loss: 0.310 | Acc: 89.342% (8777/9824): 100%|██████████| 307/307 [00:23<00:00, 12.97it/s]\n",
      "( 54/100) Loss: 0.684 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:11<00:00, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.31 acc_avg:84.431% acc:89.342% time:24s\n",
      "Testing loss:0.684 acc_avg:79.595% acc:85.332% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 55/100) Loss: 0.291 | Acc: 90.197% (8861/9824): 100%|██████████| 307/307 [00:23<00:00, 13.07it/s]\n",
      "( 55/100) Loss: 0.684 | Acc: 84.481% (2085/2468): 100%|██████████| 155/155 [00:11<00:00, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.291 acc_avg:85.757% acc:90.197% time:23s\n",
      "Testing loss:0.684 acc_avg:80.473% acc:84.481% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 56/100) Loss: 0.312 | Acc: 89.414% (8784/9824): 100%|██████████| 307/307 [00:23<00:00, 13.03it/s]\n",
      "( 56/100) Loss: 0.631 | Acc: 85.859% (2119/2468): 100%|██████████| 155/155 [00:11<00:00, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.312 acc_avg:84.409% acc:89.414% time:23s\n",
      "Testing loss:0.631 acc_avg:80.203% acc:85.859% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 57/100) Loss: 0.298 | Acc: 90.055% (8847/9824): 100%|██████████| 307/307 [00:23<00:00, 13.09it/s]\n",
      "( 57/100) Loss: 0.675 | Acc: 84.360% (2082/2468): 100%|██████████| 155/155 [00:11<00:00, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.298 acc_avg:85.538% acc:90.055% time:23s\n",
      "Testing loss:0.675 acc_avg:78.67% acc:84.36% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 58/100) Loss: 0.297 | Acc: 89.770% (8819/9824): 100%|██████████| 307/307 [00:23<00:00, 13.04it/s]\n",
      "( 58/100) Loss: 0.652 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:11<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.297 acc_avg:85.428% acc:89.77% time:23s\n",
      "Testing loss:0.652 acc_avg:79.332% acc:85.494% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 59/100) Loss: 0.292 | Acc: 90.279% (8869/9824): 100%|██████████| 307/307 [00:23<00:00, 13.12it/s]\n",
      "( 59/100) Loss: 0.666 | Acc: 84.724% (2091/2468): 100%|██████████| 155/155 [00:11<00:00, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.292 acc_avg:85.93% acc:90.279% time:23s\n",
      "Testing loss:0.666 acc_avg:79.291% acc:84.724% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 60/100) Loss: 0.277 | Acc: 90.401% (8881/9824): 100%|██████████| 307/307 [00:23<00:00, 12.93it/s]\n",
      "( 60/100) Loss: 0.674 | Acc: 85.575% (2112/2468): 100%|██████████| 155/155 [00:11<00:00, 13.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.277 acc_avg:86.23% acc:90.401% time:24s\n",
      "Testing loss:0.674 acc_avg:80.565% acc:85.575% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 61/100) Loss: 0.278 | Acc: 90.656% (8906/9824): 100%|██████████| 307/307 [00:22<00:00, 13.58it/s]\n",
      "( 61/100) Loss: 0.701 | Acc: 84.238% (2079/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.278 acc_avg:86.491% acc:90.656% time:22s\n",
      "Testing loss:0.701 acc_avg:79.002% acc:84.238% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 62/100) Loss: 0.284 | Acc: 90.024% (8844/9824): 100%|██████████| 307/307 [00:22<00:00, 13.53it/s]\n",
      "( 62/100) Loss: 0.685 | Acc: 85.737% (2116/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.284 acc_avg:85.578% acc:90.024% time:23s\n",
      "Testing loss:0.685 acc_avg:79.811% acc:85.737% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 63/100) Loss: 0.267 | Acc: 90.564% (8897/9824): 100%|██████████| 307/307 [00:22<00:00, 13.47it/s]\n",
      "( 63/100) Loss: 0.689 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:11<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.267 acc_avg:86.124% acc:90.564% time:23s\n",
      "Testing loss:0.689 acc_avg:79.402% acc:85.049% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 64/100) Loss: 0.273 | Acc: 90.666% (8907/9824): 100%|██████████| 307/307 [00:22<00:00, 13.42it/s]\n",
      "( 64/100) Loss: 0.709 | Acc: 84.643% (2089/2468): 100%|██████████| 155/155 [00:11<00:00, 14.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.273 acc_avg:86.486% acc:90.666% time:23s\n",
      "Testing loss:0.709 acc_avg:79.09% acc:84.643% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 65/100) Loss: 0.251 | Acc: 91.480% (8987/9824): 100%|██████████| 307/307 [00:22<00:00, 13.46it/s]\n",
      "( 65/100) Loss: 0.690 | Acc: 85.049% (2099/2468): 100%|██████████| 155/155 [00:11<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.251 acc_avg:87.464% acc:91.48% time:23s\n",
      "Testing loss:0.69 acc_avg:79.469% acc:85.049% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 66/100) Loss: 0.260 | Acc: 91.388% (8978/9824): 100%|██████████| 307/307 [00:22<00:00, 13.41it/s]\n",
      "( 66/100) Loss: 0.687 | Acc: 85.494% (2110/2468): 100%|██████████| 155/155 [00:11<00:00, 13.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.26 acc_avg:87.313% acc:91.388% time:23s\n",
      "Testing loss:0.687 acc_avg:80.206% acc:85.494% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 67/100) Loss: 0.251 | Acc: 91.134% (8953/9824): 100%|██████████| 307/307 [00:22<00:00, 13.43it/s]\n",
      "( 67/100) Loss: 0.689 | Acc: 85.373% (2107/2468): 100%|██████████| 155/155 [00:11<00:00, 13.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.251 acc_avg:86.935% acc:91.134% time:23s\n",
      "Testing loss:0.689 acc_avg:79.586% acc:85.373% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 68/100) Loss: 0.251 | Acc: 91.063% (8946/9824): 100%|██████████| 307/307 [00:22<00:00, 13.44it/s]\n",
      "( 68/100) Loss: 0.675 | Acc: 85.413% (2108/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.251 acc_avg:87.078% acc:91.063% time:23s\n",
      "Testing loss:0.675 acc_avg:80.256% acc:85.413% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 69/100) Loss: 0.254 | Acc: 91.531% (8992/9824): 100%|██████████| 307/307 [00:22<00:00, 13.50it/s]\n",
      "( 69/100) Loss: 0.685 | Acc: 84.887% (2095/2468): 100%|██████████| 155/155 [00:11<00:00, 13.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.254 acc_avg:87.497% acc:91.531% time:23s\n",
      "Testing loss:0.685 acc_avg:79.111% acc:84.887% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 70/100) Loss: 0.246 | Acc: 91.368% (8976/9824): 100%|██████████| 307/307 [00:23<00:00, 13.13it/s]\n",
      "( 70/100) Loss: 0.692 | Acc: 85.251% (2104/2468): 100%|██████████| 155/155 [00:11<00:00, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.246 acc_avg:87.619% acc:91.368% time:23s\n",
      "Testing loss:0.692 acc_avg:79.878% acc:85.251% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 71/100) Loss: 0.245 | Acc: 91.623% (9001/9824): 100%|██████████| 307/307 [00:23<00:00, 13.02it/s]\n",
      "( 71/100) Loss: 0.700 | Acc: 85.535% (2111/2468): 100%|██████████| 155/155 [00:11<00:00, 13.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.245 acc_avg:87.862% acc:91.623% time:23s\n",
      "Testing loss:0.7 acc_avg:79.94% acc:85.535% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 72/100) Loss: 0.231 | Acc: 92.040% (9042/9824): 100%|██████████| 307/307 [00:23<00:00, 12.95it/s]\n",
      "( 72/100) Loss: 0.672 | Acc: 85.656% (2114/2468): 100%|██████████| 155/155 [00:11<00:00, 13.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.231 acc_avg:88.11% acc:92.04% time:24s\n",
      "Testing loss:0.672 acc_avg:80.415% acc:85.656% time:12s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 73/100) Loss: 0.241 | Acc: 91.958% (9034/9824): 100%|██████████| 307/307 [00:23<00:00, 12.82it/s]\n",
      "( 73/100) Loss: 0.711 | Acc: 85.656% (2114/2468): 100%|██████████| 155/155 [00:11<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.241 acc_avg:88.602% acc:91.958% time:24s\n",
      "Testing loss:0.711 acc_avg:79.694% acc:85.656% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 74/100) Loss: 0.228 | Acc: 91.867% (9025/9824): 100%|██████████| 307/307 [00:23<00:00, 13.22it/s]\n",
      "( 74/100) Loss: 0.698 | Acc: 85.818% (2118/2468): 100%|██████████| 155/155 [00:11<00:00, 13.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.228 acc_avg:88.214% acc:91.867% time:23s\n",
      "Testing loss:0.698 acc_avg:80.002% acc:85.818% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 75/100) Loss: 0.223 | Acc: 91.938% (9032/9824): 100%|██████████| 307/307 [00:23<00:00, 13.09it/s]\n",
      "( 75/100) Loss: 0.722 | Acc: 85.575% (2112/2468): 100%|██████████| 155/155 [00:11<00:00, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.223 acc_avg:88.357% acc:91.938% time:23s\n",
      "Testing loss:0.722 acc_avg:80.361% acc:85.575% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 76/100) Loss: 0.226 | Acc: 92.121% (9050/9824): 100%|██████████| 307/307 [00:23<00:00, 13.15it/s]\n",
      "( 76/100) Loss: 0.718 | Acc: 85.737% (2116/2468): 100%|██████████| 155/155 [00:11<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.226 acc_avg:88.813% acc:92.121% time:23s\n",
      "Testing loss:0.718 acc_avg:80.332% acc:85.737% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 77/100) Loss: 0.224 | Acc: 92.162% (9054/9824): 100%|██████████| 307/307 [00:23<00:00, 13.16it/s]\n",
      "( 77/100) Loss: 0.719 | Acc: 85.900% (2120/2468): 100%|██████████| 155/155 [00:11<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.224 acc_avg:88.688% acc:92.162% time:23s\n",
      "Testing loss:0.719 acc_avg:80.957% acc:85.9% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 78/100) Loss: 0.217 | Acc: 92.651% (9102/9824): 100%|██████████| 307/307 [00:23<00:00, 13.33it/s]\n",
      "( 78/100) Loss: 0.715 | Acc: 85.616% (2113/2468): 100%|██████████| 155/155 [00:11<00:00, 13.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.217 acc_avg:89.296% acc:92.651% time:23s\n",
      "Testing loss:0.715 acc_avg:80.357% acc:85.616% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 79/100) Loss: 0.216 | Acc: 92.447% (9082/9824): 100%|██████████| 307/307 [00:23<00:00, 13.13it/s]\n",
      "( 79/100) Loss: 0.738 | Acc: 85.251% (2104/2468): 100%|██████████| 155/155 [00:11<00:00, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.216 acc_avg:89.274% acc:92.447% time:23s\n",
      "Testing loss:0.738 acc_avg:80.057% acc:85.251% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 80/100) Loss: 0.204 | Acc: 92.814% (9118/9824): 100%|██████████| 307/307 [00:23<00:00, 12.90it/s]\n",
      "( 80/100) Loss: 0.738 | Acc: 85.211% (2103/2468): 100%|██████████| 155/155 [00:11<00:00, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.204 acc_avg:89.689% acc:92.814% time:24s\n",
      "Testing loss:0.738 acc_avg:79.532% acc:85.211% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 81/100) Loss: 0.212 | Acc: 92.569% (9094/9824): 100%|██████████| 307/307 [00:23<00:00, 12.89it/s]\n",
      "( 81/100) Loss: 0.719 | Acc: 85.170% (2102/2468): 100%|██████████| 155/155 [00:11<00:00, 13.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.212 acc_avg:88.976% acc:92.569% time:24s\n",
      "Testing loss:0.719 acc_avg:79.87% acc:85.17% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 82/100) Loss: 0.216 | Acc: 92.284% (9066/9824): 100%|██████████| 307/307 [00:23<00:00, 12.98it/s]\n",
      "( 82/100) Loss: 0.713 | Acc: 85.332% (2106/2468): 100%|██████████| 155/155 [00:11<00:00, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.216 acc_avg:88.726% acc:92.284% time:24s\n",
      "Testing loss:0.713 acc_avg:79.783% acc:85.332% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 83/100) Loss: 0.217 | Acc: 92.172% (9055/9824): 100%|██████████| 307/307 [00:23<00:00, 13.04it/s]\n",
      "( 83/100) Loss: 0.727 | Acc: 85.697% (2115/2468): 100%|██████████| 155/155 [00:11<00:00, 13.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.217 acc_avg:88.339% acc:92.172% time:23s\n",
      "Testing loss:0.727 acc_avg:80.211% acc:85.697% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 84/100) Loss: 0.204 | Acc: 92.579% (9095/9824): 100%|██████████| 307/307 [00:23<00:00, 12.98it/s]\n",
      "( 84/100) Loss: 0.715 | Acc: 85.818% (2118/2468): 100%|██████████| 155/155 [00:11<00:00, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.204 acc_avg:89.257% acc:92.579% time:24s\n",
      "Testing loss:0.715 acc_avg:80.466% acc:85.818% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 85/100) Loss: 0.205 | Acc: 92.417% (9079/9824): 100%|██████████| 307/307 [00:23<00:00, 13.06it/s]\n",
      "( 85/100) Loss: 0.739 | Acc: 85.859% (2119/2468): 100%|██████████| 155/155 [00:11<00:00, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.205 acc_avg:88.752% acc:92.417% time:23s\n",
      "Testing loss:0.739 acc_avg:81.036% acc:85.859% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 86/100) Loss: 0.196 | Acc: 92.895% (9126/9824): 100%|██████████| 307/307 [00:23<00:00, 13.09it/s]\n",
      "( 86/100) Loss: 0.746 | Acc: 85.818% (2118/2468): 100%|██████████| 155/155 [00:11<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.196 acc_avg:89.61% acc:92.895% time:23s\n",
      "Testing loss:0.746 acc_avg:80.915% acc:85.818% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 87/100) Loss: 0.199 | Acc: 92.875% (9124/9824): 100%|██████████| 307/307 [00:22<00:00, 13.49it/s]\n",
      "( 87/100) Loss: 0.732 | Acc: 86.102% (2125/2468): 100%|██████████| 155/155 [00:11<00:00, 13.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.199 acc_avg:89.522% acc:92.875% time:23s\n",
      "Testing loss:0.732 acc_avg:80.782% acc:86.102% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "( 88/100) Loss: 0.195 | Acc: 93.119% (9148/9824): 100%|██████████| 307/307 [00:22<00:00, 13.51it/s]\n",
      "( 88/100) Loss: 0.751 | Acc: 85.859% (2119/2468): 100%|██████████| 155/155 [00:11<00:00, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.195 acc_avg:89.954% acc:93.119% time:23s\n",
      "Testing loss:0.751 acc_avg:80.473% acc:85.859% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 89/100) Loss: 0.191 | Acc: 92.976% (9134/9824): 100%|██████████| 307/307 [00:22<00:00, 13.56it/s]\n",
      "( 89/100) Loss: 0.749 | Acc: 86.021% (2123/2468): 100%|██████████| 155/155 [00:11<00:00, 13.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.191 acc_avg:89.868% acc:92.976% time:22s\n",
      "Testing loss:0.749 acc_avg:80.628% acc:86.021% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 90/100) Loss: 0.194 | Acc: 92.803% (9117/9824): 100%|██████████| 307/307 [00:22<00:00, 13.51it/s]\n",
      "( 90/100) Loss: 0.739 | Acc: 85.940% (2121/2468): 100%|██████████| 155/155 [00:11<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.194 acc_avg:89.514% acc:92.803% time:23s\n",
      "Testing loss:0.739 acc_avg:80.949% acc:85.94% time:11s [best test acc: 86.264%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 91/100) Loss: 0.195 | Acc: 92.834% (9120/9824): 100%|██████████| 307/307 [00:22<00:00, 13.43it/s]\n",
      "( 91/100) Loss: 0.736 | Acc: 86.305% (2130/2468): 100%|██████████| 155/155 [00:11<00:00, 13.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.195 acc_avg:89.464% acc:92.834% time:23s\n",
      "Testing loss:0.736 acc_avg:80.898% acc:86.305% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 92/100) Loss: 0.209 | Acc: 92.671% (9104/9824): 100%|██████████| 307/307 [00:22<00:00, 13.36it/s]\n",
      "( 92/100) Loss: 0.751 | Acc: 85.656% (2114/2468): 100%|██████████| 155/155 [00:11<00:00, 13.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.209 acc_avg:89.725% acc:92.671% time:23s\n",
      "Testing loss:0.751 acc_avg:80.286% acc:85.656% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 93/100) Loss: 0.197 | Acc: 92.834% (9120/9824): 100%|██████████| 307/307 [00:22<00:00, 13.55it/s]\n",
      "( 93/100) Loss: 0.754 | Acc: 86.224% (2128/2468): 100%|██████████| 155/155 [00:11<00:00, 13.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.197 acc_avg:89.681% acc:92.834% time:23s\n",
      "Testing loss:0.754 acc_avg:80.661% acc:86.224% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 94/100) Loss: 0.196 | Acc: 92.997% (9136/9824): 100%|██████████| 307/307 [00:23<00:00, 12.88it/s]\n",
      "( 94/100) Loss: 0.752 | Acc: 85.413% (2108/2468): 100%|██████████| 155/155 [00:11<00:00, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.196 acc_avg:89.98% acc:92.997% time:24s\n",
      "Testing loss:0.752 acc_avg:79.994% acc:85.413% time:12s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 95/100) Loss: 0.190 | Acc: 93.394% (9175/9824): 100%|██████████| 307/307 [00:23<00:00, 12.97it/s]\n",
      "( 95/100) Loss: 0.754 | Acc: 85.940% (2121/2468): 100%|██████████| 155/155 [00:11<00:00, 13.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.19 acc_avg:90.24% acc:93.394% time:24s\n",
      "Testing loss:0.754 acc_avg:80.582% acc:85.94% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 96/100) Loss: 0.195 | Acc: 92.905% (9127/9824): 100%|██████████| 307/307 [00:23<00:00, 13.00it/s]\n",
      "( 96/100) Loss: 0.738 | Acc: 85.818% (2118/2468): 100%|██████████| 155/155 [00:11<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.195 acc_avg:89.777% acc:92.905% time:24s\n",
      "Testing loss:0.738 acc_avg:80.428% acc:85.818% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 97/100) Loss: 0.188 | Acc: 93.414% (9177/9824): 100%|██████████| 307/307 [00:23<00:00, 13.02it/s]\n",
      "( 97/100) Loss: 0.748 | Acc: 86.102% (2125/2468): 100%|██████████| 155/155 [00:11<00:00, 13.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.188 acc_avg:90.204% acc:93.414% time:23s\n",
      "Testing loss:0.748 acc_avg:80.978% acc:86.102% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 98/100) Loss: 0.188 | Acc: 93.170% (9153/9824): 100%|██████████| 307/307 [00:23<00:00, 13.05it/s]\n",
      "( 98/100) Loss: 0.742 | Acc: 85.737% (2116/2468): 100%|██████████| 155/155 [00:11<00:00, 13.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.188 acc_avg:90.099% acc:93.17% time:23s\n",
      "Testing loss:0.742 acc_avg:80.628% acc:85.737% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "( 99/100) Loss: 0.187 | Acc: 93.149% (9151/9824): 100%|██████████| 307/307 [00:23<00:00, 12.95it/s]\n",
      "( 99/100) Loss: 0.755 | Acc: 85.940% (2121/2468): 100%|██████████| 155/155 [00:11<00:00, 13.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:0.187 acc_avg:90.147% acc:93.149% time:24s\n",
      "Testing loss:0.755 acc_avg:81.078% acc:85.94% time:11s [best test acc: 86.305%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training (Taken from PointMLP-PyTorch)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 100 # Number of epochs\n",
    "eval_every = 10 # Evaluate every 'eval_every' epochs\n",
    "max_lr = 0.1 # Maximum learning rate\n",
    "min_lr = 0.005 # Minimum learning rate\n",
    "\n",
    "best_test_acc = 0.  # best test accuracy\n",
    "best_train_acc = 0.\n",
    "best_test_acc_avg = 0.\n",
    "best_train_acc_avg = 0.\n",
    "best_test_loss = float(\"inf\")\n",
    "best_train_loss = float(\"inf\")\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Model\n",
    "net = MomentNet(3, [256, 128], 40, degree = 14, poly_type = 'chebyshev').to(device)\n",
    "print(net.num_poly)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=max_lr, momentum=0.9, weight_decay=0)\n",
    "scheduler = CosineAnnealingLR(optimizer, num_epochs, eta_min=min_lr, last_epoch=start_epoch - 1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    train_out = train(net, train_loader, optimizer, criterion, epoch, num_epochs, device)  # {\"loss\", \"acc\", \"acc_avg\", \"time\"}\n",
    "    test_out = validate(net, test_loader, criterion, epoch, num_epochs, device)\n",
    "    scheduler.step()\n",
    "    if test_out[\"acc\"] > best_test_acc:\n",
    "        best_test_acc = test_out[\"acc\"]\n",
    "        is_best = True\n",
    "    else:\n",
    "        is_best = False\n",
    "\n",
    "    best_test_acc = test_out[\"acc\"] if (test_out[\"acc\"] > best_test_acc) else best_test_acc\n",
    "    best_train_acc = train_out[\"acc\"] if (train_out[\"acc\"] > best_train_acc) else best_train_acc\n",
    "    best_test_acc_avg = test_out[\"acc_avg\"] if (test_out[\"acc_avg\"] > best_test_acc_avg) else best_test_acc_avg\n",
    "    best_train_acc_avg = train_out[\"acc_avg\"] if (train_out[\"acc_avg\"] > best_train_acc_avg) else best_train_acc_avg\n",
    "    best_test_loss = test_out[\"loss\"] if (test_out[\"loss\"] < best_test_loss) else best_test_loss\n",
    "    best_train_loss = train_out[\"loss\"] if (train_out[\"loss\"] < best_train_loss) else best_train_loss\n",
    "    \n",
    "    print(\n",
    "        f\"Training loss:{train_out['loss']} acc_avg:{train_out['acc_avg']}% acc:{train_out['acc']}% time:{train_out['time']}s\")\n",
    "    print(\n",
    "        f\"Testing loss:{test_out['loss']} acc_avg:{test_out['acc_avg']}% \"\n",
    "        f\"acc:{test_out['acc']}% time:{test_out['time']}s [best test acc: {best_test_acc}%]\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
